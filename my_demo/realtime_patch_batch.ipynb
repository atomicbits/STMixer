{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6966f42",
   "metadata": {},
   "source": [
    "## Frame-by-Frame processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96ff5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import cv2\n",
    "import math\n",
    "import numpy as np\n",
    "import moviepy \n",
    "import torch\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "from rt_alphaction.config import cfg\n",
    "from rt_alphaction.modeling.detector import build_detection_model\n",
    "from rt_alphaction.utils.checkpoint import ActionCheckpointer\n",
    "from rt_alphaction.utils.comm import get_world_size\n",
    "\n",
    "\n",
    "from my_utils.gen_utils import create_experiment_folder\n",
    "from my_utils.video_processing import get_video_info, get_frame_from_video\n",
    "from my_utils.slicing import get_slice_bboxes, generate_sliding_window_gif\n",
    "\n",
    "from my_utils.video_processing import segment_crop_video\n",
    "from my_utils.ava_preprocessing import ava_preprocessing_cv2, clip_constructor, prepare_collated_batches, prepare_collated_batches_v2\n",
    "from my_utils.ava_postprocessing import concatenate_results\n",
    "from my_utils.visualization import action_visualizer_frame_index\n",
    "\n",
    "from my_utils.gen_utils import parse_label_file\n",
    "\n",
    "from my_utils.ava_postprocessing import clip_boxes_tensor, map_bbox_from_prep_to_crop, map_bbox_from_crop_to_orig\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3222f3",
   "metadata": {},
   "source": [
    "### 1. CONFIG\n",
    "#### 1.1 Main Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba83991c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'VMAEv2'\n",
    "\n",
    "\n",
    "person_threshold = 0.3 # confidence threshold on actor. 0.6 is the defualt\n",
    "sampling_rate = 3 # sampling rate: 4 is the defualt\n",
    "top_k = 5 # number of actions per person\n",
    "video_path = '../input_dir/Fighting_14.mp4'\n",
    "stream = False\n",
    "\n",
    "slice_height = 600\n",
    "slice_width = 800\n",
    "overlap_ratio = 0\n",
    "\n",
    "starting_frame_index = 900\n",
    "length_input = 300\n",
    "\n",
    "exp_dict = {'model_name': model_name,\n",
    "            'model_params': {'person_threshold': person_threshold, \n",
    "                             'sampling_rate': sampling_rate},\n",
    "            'orig_post_processing':{'top_k': top_k},\n",
    "            'aggregation': {'method': {}, \n",
    "                            'params': {}},\n",
    "            'video_path': video_path,\n",
    "            'slicing_params': {'slice_height': slice_height, \n",
    "                               'slice_width': slice_width, \n",
    "                               'overlap_ratio':overlap_ratio},\n",
    "            'video_params': {'st_frame_index': starting_frame_index, \n",
    "                             'length_input':length_input\n",
    "                             }\n",
    "           }\n",
    "\n",
    "exp_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656995dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_name = os.path.basename(video_path).split('.')[0]\n",
    "#output_directory = f'../output_dir/{video_name}/{model_name}/patch_batch/' \n",
    "#output_directory = create_experiment_folder(output_directory, 'exp')\n",
    "#output_directory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265cbefc",
   "metadata": {},
   "source": [
    "#### 1.2 Model Config Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87183304",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cfg_create(model_name, person_threshold, sampling_rate, test_videos_batch=1, num_workers=1):\n",
    "    \n",
    "    if model_name == 'VMAEv2':\n",
    "        config_file = '../config_files/VMAEv2-ViTB-16x4.yaml'\n",
    "        \n",
    "    if model_name == 'VMAE':\n",
    "        config_file = '../config_files/VMAE-ViTB-16x4.yaml'\n",
    "        \n",
    "    cfg.merge_from_file(config_file)\n",
    "    \n",
    "    # change model weight path\n",
    "    if model_name == 'VMAEv2':\n",
    "        cfg.merge_from_list([\"MODEL.WEIGHT\", \"../checkpoints/VMAEv2_ViTB_16x4.pth\"])\n",
    "    if model_name == 'VMAE':\n",
    "        cfg.merge_from_list([\"MODEL.WEIGHT\", \"../checkpoints/VMAE_ViTB_16x4.pth\"])\n",
    "\n",
    "    # change output dir\n",
    "    cfg.merge_from_list([\"OUTPUT_DIR\", \"../output_dir/\"])\n",
    "\n",
    "    # change person threshold\n",
    "    cfg.merge_from_list([\"MODEL.STM.PERSON_THRESHOLD\", person_threshold])\n",
    "\n",
    "    # change sampling rate\n",
    "    cfg.merge_from_list([\"DATA.SAMPLING_RATE\", sampling_rate])\n",
    "\n",
    "    # change path for data_dir\n",
    "    cfg.merge_from_list([\"DATA.PATH_TO_DATA_DIR\", \"/work/ava\"])\n",
    "\n",
    "    # folder name of annotations\n",
    "    cfg.merge_from_list([\"AVA.ANNOTATION_DIR\", \"annotations/\"])\n",
    "\n",
    "    # file name of  frame_lists\n",
    "    cfg.merge_from_list([\"AVA.TRAIN_LISTS\", ['sample.csv']])\n",
    "    cfg.merge_from_list([\"AVA.TEST_LISTS\", ['sample.csv']])\n",
    "\n",
    "    # file name of predicted_bboxes\n",
    "    cfg.merge_from_list([\"AVA.TRAIN_GT_BOX_LISTS\", ['ava_sample_predicted_boxes.csv']])\n",
    "    cfg.merge_from_list([\"AVA.TEST_GT_BOX_LISTS\", ['ava_sample_predicted_boxes.csv']])\n",
    "\n",
    "    # file name of exlusions\n",
    "    cfg.merge_from_list([\"AVA.EXCLUSION_FILE\", 'ava_sample_train_excluded_timestamps_v2.2.csv'])\n",
    "\n",
    "    # number of batches in test scenario\n",
    "    cfg.merge_from_list([\"TEST.VIDEOS_PER_BATCH\", test_videos_batch])\n",
    "\n",
    "    # number of workers\n",
    "    cfg.merge_from_list([\"DATALOADER.NUM_WORKERS\", num_workers])\n",
    "    \n",
    "    \n",
    "    return cfg\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4754c7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = cfg_create(model_name, person_threshold, sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00476179",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_detection_model(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6187164d",
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = True\n",
    "if debug:\n",
    "    # The shape of model input should be divisible into this. Otherwise, padding 0 to left and bottum. \n",
    "    print(\"cfg.DATALOADER.SIZE_DIVISIBILITY: \", cfg.DATALOADER.SIZE_DIVISIBILITY)\n",
    "    \n",
    "    # Sampling rate in constructing the clips.\n",
    "    self_sample_rate =  cfg.DATA.SAMPLING_RATE\n",
    "    print(\"cfg.DATA.SAMPLING_RATE: \", cfg.DATA.SAMPLING_RATE)\n",
    "    \n",
    "    # Length of clip\n",
    "    self_video_length = cfg.DATA.NUM_FRAMES\n",
    "    print(\"cfg.DATA.NUM_FRAMES: \", cfg.DATA.NUM_FRAMES)\n",
    "    \n",
    "    # Length of sequence frames from which a clip is constructed.\n",
    "    self_seq_len = self_video_length * self_sample_rate\n",
    "    print(\"self_seq_len: \", self_seq_len)\n",
    "    \n",
    "    self_num_classes = cfg.MODEL.STM.ACTION_CLASSES\n",
    "    print(\"cfg.MODEL.STM.ACTION_CLASSES: \", self_num_classes)\n",
    "    \n",
    "    # Augmentation params.\n",
    "    self_data_mean = cfg.DATA.MEAN\n",
    "    self_data_std = cfg.DATA.STD\n",
    "    self_use_bgr = cfg.AVA.BGR\n",
    "    print(\"Augmentation params: \", self_data_mean, self_data_std, self_use_bgr)\n",
    "    \n",
    "    self_jitter_min_scale = cfg.DATA.TEST_MIN_SCALES\n",
    "    self_jitter_max_scale = cfg.DATA.TEST_MAX_SCALE\n",
    "    self_test_force_flip = cfg.AVA.TEST_FORCE_FLIP\n",
    "\n",
    "    print(\"scale and flip params\", self_jitter_min_scale, self_jitter_max_scale, self_test_force_flip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770e0c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = cfg.DATA.NUM_FRAMES * cfg.DATA.SAMPLING_RATE\n",
    "print(seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9b339d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec664a1",
   "metadata": {},
   "source": [
    "### 5. loading weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c36d27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = cfg.OUTPUT_DIR\n",
    "output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe6cd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointer = ActionCheckpointer(cfg, model, save_dir=output_dir)\n",
    "checkpointer.load(cfg.MODEL.WEIGHT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3054e5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_gpus = int(os.environ[\"WORLD_SIZE\"]) if \"WORLD_SIZE\" in os.environ else 1\n",
    "num_gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d68c498",
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_active = cfg.MODEL.STM.MEM_ACTIVE\n",
    "mem_active  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cf88b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6070b453",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_devices = get_world_size()\n",
    "num_devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbb96ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923b3628",
   "metadata": {},
   "source": [
    "### 3. VIDEO Info and Slicing Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10875a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_info = get_video_info(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344b1e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d0e529",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_dict['video_params'].update(video_info)\n",
    "exp_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a797bbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_height = video_info['height']\n",
    "frame_width = video_info['width']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420b041b",
   "metadata": {},
   "outputs": [],
   "source": [
    "patches_coordinates = get_slice_bboxes(frame_height, frame_width, slice_height, slice_width, False, overlap_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e6e784",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(patches_coordinates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda0e5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "patches_coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d900ba78",
   "metadata": {},
   "outputs": [],
   "source": [
    "patches_shapes = [[item[2] - item[0], item[3] - item[1]] for item in patches_coordinates]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c5adc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "patches_shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc600d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_frame(frame, patches_coordinates):\n",
    "    frame_slices = [frame[y1:y2, x1:x2] for x1, y1, x2, y2 in patches_coordinates]\n",
    "    return frame_slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf94c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bda37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_threshold(scores, threshold):\n",
    "    # Create a boolean mask where True indicates scores higher than the threshold\n",
    "    mask = scores > threshold\n",
    "    \n",
    "    # Apply the mask to filter out detections\n",
    "    detections = []\n",
    "    for batch_idx in range(scores.shape[0]):\n",
    "        batch_detections = torch.nonzero(mask[batch_idx]).squeeze(-1).tolist()\n",
    "        detections.append(batch_detections)\n",
    "    \n",
    "    return detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed3b860",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_selected_detections_list(all_detections_tensor, indices_list):\n",
    "    selected_detections_list = []\n",
    "    \n",
    "    for batch_idx, indices in enumerate(indices_list):\n",
    "        selected_detections = all_detections_tensor[batch_idx, indices]\n",
    "        selected_detections_list.append(selected_detections)\n",
    "    \n",
    "    return selected_detections_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef51a15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_thresholding(inter_class_logits, inter_pred_bboxes, inter_action_logits, person_threshold):\n",
    "    \n",
    "    # applying softmax to get objectness score\n",
    "    obj_scores = F.softmax(inter_class_logits[-1], dim=-1)[:, :, 0] # cuda torch of shape nr_patches x 100\n",
    "    \n",
    "    # applying sigmoid on last item to get action scores\n",
    "    action_scores = torch.sigmoid(inter_action_logits[-1])  # cuda torch of shape nr_patches x 100 x 80\n",
    "    \n",
    "    # list of valid detection indices (from 100) at each path\n",
    "    list_val_det_indices = apply_threshold(obj_scores, person_threshold) # list of length nr_pathces\n",
    "    \n",
    "    # list of obj scores of valid detections at each patch\n",
    "    selected_obj_scores = create_selected_detections_list(obj_scores, list_val_det_indices)\n",
    "    \n",
    "    # list of actions scores of valid detections at each patch\n",
    "    selected_action_scores = create_selected_detections_list(action_scores, list_val_det_indices)\n",
    "    \n",
    "    # list of bbox actions of valid detections at each patch\n",
    "    selected_bboxes = create_selected_detections_list(inter_pred_bboxes[-1], list_val_det_indices)\n",
    "    \n",
    "    return selected_obj_scores, selected_action_scores, selected_bboxes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a90a2d",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0665f718",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "\n",
    "frames_tensor_list = []\n",
    "\n",
    "central_frames = [] # list of central frames\n",
    "central_frames_id = []\n",
    "starting_inferece = False\n",
    "\n",
    "frame_id = -1\n",
    "\n",
    "\n",
    "buffer_size = sampling_rate * (cfg.DATA.NUM_FRAMES - 1) + 1 # 46 for sampling 3\n",
    "\n",
    "temp_results_dict = {}\n",
    "\n",
    "# Read frames from video\n",
    "while True:\n",
    "    \n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    frame_id += 1\n",
    "    \n",
    "    # add first central frame or add new frame if we have already added first central frame\n",
    "    if frame_id == sampling_rate * (cfg.DATA.NUM_FRAMES // 2) or len(central_frames) != 0:\n",
    "        central_frames.append(frame)\n",
    "        central_frames_id.append(frame_id)\n",
    "    \n",
    "        \n",
    "    # list of slices of current frame. length of list: n_patches. shape of each frame: H_patch, W_patch, 3\n",
    "    frame_slices = crop_frame(frame, patches_coordinates)\n",
    "    \n",
    "    # result of preprocessing of patches of current frame: torch of shape [3, n_patches, 256, 455]\n",
    "    frames_prep = ava_preprocessing_cv2(frame_slices, cfg) \n",
    "    \n",
    "    # reshape preprocessed patches of current frame to [nr_patch, 3, 256, 307] and add it to the list\n",
    "    frames_tensor_list.append(frames_prep.permute(1, 0, 2, 3))\n",
    "    \n",
    "    # \n",
    "    if len(frames_tensor_list) == buffer_size:\n",
    "        \n",
    "        temp_results_dict[central_frames_id[0]] = []\n",
    "\n",
    "        # creating a video of shape nr_patches x buffer_size(46) x 256 x 307\n",
    "        video_tensor = torch.stack(frames_tensor_list).permute(1, 2, 0, 3, 4)\n",
    "        \n",
    "        # creating a clip of shape nr_patches x 3 x cfg.DATA.NUM_FRAMES x 256 x 307\n",
    "        clip_tensor = video_tensor[:,:, 0::sampling_rate,:,:] \n",
    "        \n",
    "        # passing clip to collate: list of length nr_patches, each item is list of length 7 (slow_video, ...) \n",
    "        list_collated_batches = prepare_collated_batches_v2(clip_tensor, cfg)\n",
    "        \n",
    "        # creating a tensor of shape: nr_patches x 3 x cfg.DATA.NUM_FRAMES x 256 x 320 \n",
    "        slow_video = torch.stack([torch.squeeze(item[0]) for item in list_collated_batches])\n",
    "        \n",
    "        # hardcoding fast_video None for MAE-STMixer\n",
    "        fast_video = None\n",
    "        \n",
    "        whwh = torch.stack([torch.squeeze(item[2]) for item in list_collated_batches])\n",
    "        boxes = list_collated_batches[0][3]\n",
    "        labels = list_collated_batches[0][4]\n",
    "        \n",
    "        slow_video = slow_video.to(device)\n",
    "        if fast_video is not None:\n",
    "            fast_video = fast_video.to(device)\n",
    "        whwh = whwh.to(device)\n",
    "    \n",
    "        # INFERENCE\n",
    "        with torch.no_grad():\n",
    "            inter_class_logits, inter_pred_bboxes, inter_action_logits, B, N = model(slow_video, fast_video, whwh, boxes, labels)\n",
    "        \n",
    "        selected_obj_scores, selected_action_scores, selected_bboxes = batch_thresholding(inter_class_logits, inter_pred_bboxes, inter_action_logits, person_threshold)\n",
    "        \n",
    "        top_values = [[] for _ in range(B)]\n",
    "        top_indices = [[] for _ in range(B)]\n",
    "        \n",
    "        output_objectness_np = [[] for _ in range(B)]\n",
    "        output_bbox_frame_np = [[] for _ in range(B)]\n",
    "        \n",
    "        top_indices_np = [[] for _ in range(B)]\n",
    "        top_values_np = [[] for _ in range(B)]\n",
    "        \n",
    "        \n",
    "        for i in range(B):\n",
    "            w = whwh[i,0].int()\n",
    "            h = whwh[i,1].int()\n",
    "            selected_bboxes[i] = clip_boxes_tensor(selected_bboxes[i], \n",
    "                                           height=h, \n",
    "                                           width =w)\n",
    "            selected_bboxes[i] = map_bbox_from_prep_to_crop(selected_bboxes[i], \n",
    "                                                    (patches_shapes[i][1], patches_shapes[i][0]), \n",
    "                                                    (h, w))\n",
    "            selected_bboxes[i] = map_bbox_from_crop_to_orig(selected_bboxes[i], patches_coordinates[i][:2])\n",
    "            \n",
    "            \n",
    "            top_values[i], top_indices[i] = torch.topk(selected_action_scores[i], k=top_k, dim=1)\n",
    "            \n",
    "            output_objectness_np[i] = np.reshape(selected_obj_scores[i].cpu().numpy(), (-1, 1))\n",
    "            output_bbox_frame_np[i] = selected_bboxes[i].cpu().numpy()\n",
    "            \n",
    "            # shifting to ava dataset labeling\n",
    "            top_indices_np[i] = top_indices[i].cpu().numpy() + 1\n",
    "            top_values_np[i] = top_values[i].cpu().numpy()\n",
    "            \n",
    "            agg_result = np.concatenate((output_objectness_np[i], \n",
    "                                         output_bbox_frame_np[i], \n",
    "                                         top_indices_np[i], \n",
    "                                         top_values_np[i]), axis=1)\n",
    "        \n",
    "                                         \n",
    "        \n",
    "            temp_results_dict[central_frames_id[0]].append(agg_result)\n",
    "            \n",
    "        \n",
    "        \n",
    "        del frames_tensor_list[0]\n",
    "        del central_frames[0]\n",
    "        del central_frames_id[0]\n",
    "        \n",
    "            \n",
    "cap.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18dc1ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_results_dict.keys()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
