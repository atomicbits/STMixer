{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "516a12ef",
   "metadata": {},
   "source": [
    "## Steps:\n",
    "1. **'make_data_loader'** [function](https://github.com/MCG-NJU/STMixer/blob/main/alphaction/dataset/build.py): creates a list of dataloaders. We get one sample batch from this data_loader.This batch contains: \n",
    "   * **'slow_clips'**: cpu-torch tensor of shape [1, 3, 16, 256, 352] with dtype = torch.float32.\n",
    "   * **'fast_clips'**: None for this config file.\n",
    "   * **'whwh'**: cpu-torch tensor of shape [1, 4]) with dtype = torch.float32.\n",
    "   * **'boxes'**: a tuple (batch size). 'boxes[0]': np array of shape Nx4, with dype = float64. These boxes are not normalized.\n",
    "   * **'metadata'**: ([0, 902],), where 902 is the starting frame index of the video (video after cutting first 15 minutes, however, frame_index is started from 0).\n",
    "   * **'clip_ids'**: (0,)\n",
    "   \n",
    "   \n",
    "2. **inside 'make_data_loader'**:\n",
    "   * ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7debadbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from alphaction.config import cfg\n",
    "from alphaction.dataset import make_data_loader\n",
    "from alphaction.engine.inference import inference\n",
    "from alphaction.modeling.detector import build_detection_model\n",
    "from alphaction.utils.checkpoint import ActionCheckpointer\n",
    "from torch.utils.collect_env import get_pretty_env_info\n",
    "from alphaction.utils.comm import synchronize, get_rank\n",
    "from alphaction.utils.logger import setup_logger\n",
    "#pytorch issuse #973\n",
    "import resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d48ad31",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = '../config_files/VMAE-ViTB-16x4.yaml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5eacb74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.merge_from_file(config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bec1db2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change model weight path\n",
    "cfg.merge_from_list([\"MODEL.WEIGHT\", \"../checkpoints/VMAE_ViTB_16x4.pth\"])\n",
    "# change output dir\n",
    "cfg.merge_from_list([\"OUTPUT_DIR\", \"../output_dir/\"])\n",
    "\n",
    "\n",
    "# change path for data_dir\n",
    "cfg.merge_from_list([\"DATA.PATH_TO_DATA_DIR\", \"/work/ava\"])\n",
    "\n",
    "# folder name of annotations\n",
    "cfg.merge_from_list([\"AVA.ANNOTATION_DIR\", \"annotations/\"])\n",
    "\n",
    "# file name of  frame_lists\n",
    "cfg.merge_from_list([\"AVA.TRAIN_LISTS\", ['sample.csv']])\n",
    "cfg.merge_from_list([\"AVA.TEST_LISTS\", ['sample.csv']])\n",
    "\n",
    "# file name of predicted_bboxes\n",
    "cfg.merge_from_list([\"AVA.TRAIN_GT_BOX_LISTS\", ['ava_sample_predicted_boxes.csv']])\n",
    "cfg.merge_from_list([\"AVA.TEST_GT_BOX_LISTS\", ['ava_sample_predicted_boxes.csv']])\n",
    "\n",
    "# file name of exlusions\n",
    "cfg.merge_from_list([\"AVA.EXCLUSION_FILE\", 'ava_sample_train_excluded_timestamps_v2.2.csv'])\n",
    "\n",
    "# number of batches in test scenario\n",
    "cfg.merge_from_list([\"TEST.VIDEOS_PER_BATCH\", 1])\n",
    "\n",
    "# number of workers\n",
    "cfg.merge_from_list([\"DATALOADER.NUM_WORKERS\", 1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601e43aa",
   "metadata": {},
   "source": [
    "### 1. Calling 'make_data_loader'\n",
    "\n",
    "'make_data_loader' method defined here:\n",
    "\n",
    "https://github.com/MCG-NJU/STMixer/blob/main/alphaction/dataset/build.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5c99667",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loaders_test = make_data_loader(cfg, is_train=False, is_distributed=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f38d0a2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data_loaders_test), len(data_loaders_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5522382",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(data_loaders_test[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebc89561",
   "metadata": {},
   "outputs": [],
   "source": [
    "slow_clips, fast_clips, whwh, boxes, label_arrs, metadata, clip_ids = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c7ddfd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 16, 256, 352]), torch.float32, device(type='cpu'), False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slow_clips.shape, slow_clips.dtype, slow_clips.device, slow_clips.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "82620434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fast_clips is None for this config: ../config_files/VMAE-ViTB-16x4.yaml\n"
     ]
    }
   ],
   "source": [
    "if fast_clips:\n",
    "    print(fast_clips.shape, fast_clips.dtype, fast_clips.device, fast_clips.requires_grad)\n",
    "else:\n",
    "    print(f\"fast_clips is None for this config: {config_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d866441",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NoneType"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(fast_clips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb9045dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[346., 256., 346., 256.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whwh # tensor([[346., 256., 346., 256.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "098a7e4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 4]), torch.float32, device(type='cpu'), False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whwh.shape, whwh.dtype, whwh.device, whwh.requires_grad\n",
    "# (torch.Size([1, 4]), torch.float32, device(type='cpu'), False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45b0565e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tuple, 1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(boxes), len(boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "156ccf09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11, 4),\n",
       " numpy.ndarray,\n",
       " dtype('float64'),\n",
       " array([112.6656,  47.36  , 162.432 , 227.072 ]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boxes[0].shape, type(boxes[0]), boxes[0].dtype, boxes[0][0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "897d7d8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tuple, 1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(label_arrs), len(label_arrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "61772a4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11, 80),\n",
       " numpy.ndarray,\n",
       " dtype('int32'),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], dtype=int32))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_arrs[0].shape, type(label_arrs[0]),  label_arrs[0].dtype, label_arrs[0][0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e639c59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 902],)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4140aa73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10590515",
   "metadata": {},
   "source": [
    "### 2. inside 'make_data_loader'\n",
    "\n",
    "steps:\n",
    "   1. setting some params.\n",
    "   2. building datasets: `datasets = build_dataset(cfg, split=split)`. We will inspect this method in next section.\n",
    "   3. For each **dataset**:\n",
    "       * create **sampler**: `sampler = make_data_sampler(dataset, shuffle, is_distributed)`\n",
    "       * create **batch_sampler**: `batch_sampler = make_batch_data_sampler(\n",
    "            dataset, sampler, aspect_grouping, videos_per_gpu, num_iters, start_iter, drop_last)`\n",
    "       * create **collator**: `collator = BatchCollator(cfg.DATALOADER.SIZE_DIVISIBILITY)`\n",
    "       * create **data_loader** based on 'torch.utils.data.DataLoader' where `num_workers = cfg.DATALOADER.NUM_WORKERS`: \n",
    "       \n",
    "       `data_loader = torch.utils.data.DataLoader(dataset, num_workers=num_workers,batch_sampler=batch_sampler, collate_fn=collator,) `"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f73a6a",
   "metadata": {},
   "source": [
    "#### 2.1 params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72641d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_distributed = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2f608c",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_train = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aae8955",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_gpus = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a23a878",
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_per_batch = cfg.TEST.VIDEOS_PER_BATCH\n",
    "videos_per_batch # 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefe955f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.TEST.VIDEOS_PER_BATCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5f7359",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (videos_per_batch % num_gpus == 0), \"TEST.VIDEOS_PER_BATCH ({}) must be divisible by the number of GPUs ({}) used.\".format(videos_per_batch, num_gpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afac5168",
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_per_gpu = videos_per_batch // num_gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990d1ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle = False if not is_distributed else True\n",
    "shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3787b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_last = False\n",
    "num_iters = None\n",
    "start_iter = 0\n",
    "split = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0059bbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group images which have similar aspect ratio. In this case, we only\n",
    "# group in two cases: those with width / height > 1, and the other way around,\n",
    "# but the code supports more general grouping strategy\n",
    "aspect_grouping = [1] if cfg.DATALOADER.ASPECT_RATIO_GROUPING else []\n",
    "aspect_grouping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab2d6be",
   "metadata": {},
   "source": [
    "#### 2.2. Calling 'buid_dataset'\n",
    "'make_data_loader' calls **['build_dataset']()**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4395f4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from alphaction.dataset.build import build_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef9e64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = build_dataset(cfg, split=split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92d149b",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45b8697",
   "metadata": {},
   "source": [
    "#### 2.3 create 'sampler' and 'batch_sampler'\n",
    "For each `dataset`, we define `sampler` and `batch_sampler`:\n",
    "* `sampler = make_data_sampler(dataset, shuffle, is_distributed)` defined [here](https://github.com/MCG-NJU/STMixer/blob/main/alphaction/dataset/build.py).\n",
    "* `batch_sampler = make_batch_data_sampler(dataset, sampler, aspect_grouping, videos_per_gpu, num_iters, start_iter, drop_last)` defined [here](https://github.com/MCG-NJU/STMixer/blob/main/alphaction/dataset/build.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e27061",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle, is_distributed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9945bb82",
   "metadata": {},
   "source": [
    "* For distributed:\n",
    "   * sampler is instantiated from the defined sampler for distributed data: `samplers.DistributedSampler(dataset, shuffle=shuffle)`\n",
    "* Else:\n",
    "   * if shuffle:\n",
    "       * calls `torch.utils.data.sampler.RandomSampler(dataset)`\n",
    "   * else:\n",
    "       * calls `sampler = torch.utils.data.sampler.SequentialSampler(dataset)` (**THIS CASE**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75160d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "aspect_grouping, videos_per_gpu, num_iters, start_iter, drop_last"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2e7c1f",
   "metadata": {},
   "source": [
    "* if `aspect_grouping`:\n",
    "    * `batch_sampler` is defined from `samplers.GroupedBatchSampler` defined [here](https://github.com/MCG-NJU/STMixer/tree/main/alphaction/dataset/samplers).\n",
    "* else:\n",
    "    * if `num_iters` is None (**THIS CASE**):\n",
    "     \n",
    "        ```batch_sampler = torch.utils.data.sampler.BatchSampler(sampler, videos_per_batch, drop_last=drop_last)```\n",
    "    * else: \n",
    "    \n",
    "    ```batch_sampler = samplers.IterationBasedBatchSampler(batch_sampler, num_iters, start_iter)```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfd61b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = cfg.DATALOADER.NUM_WORKERS\n",
    "num_workers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894f1468",
   "metadata": {},
   "source": [
    "'collator': used to define how individual samples from a dataset are batched together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadcfeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.DATALOADER.SIZE_DIVISIBILITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f223168e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collator = BatchCollator(cfg.DATALOADER.SIZE_DIVISIBILITY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd40d6dc",
   "metadata": {},
   "source": [
    "### 3. inside 'build_dataset'\n",
    "\n",
    "'build_dataset' will call 'D.Ava(cfg, split)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195c46ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.DATA.DATASETS[0] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32ce426",
   "metadata": {},
   "source": [
    "### 3. inside 'dataset.Ava' class\n",
    "\n",
    "Object from this [class](https://github.com/MCG-NJU/STMixer/blob/main/alphaction/dataset/datasets/ava_dataset.py)\n",
    "\n",
    "#### 3.1 init\n",
    "1. setting params\n",
    "2. calling `self._load_data(cfg)`\n",
    "3. setting `eval_file_paths`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a67fa1",
   "metadata": {},
   "source": [
    "##### 3.1.1 setting params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08584a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "self_cfg = cfg\n",
    "self_split = split\n",
    "\n",
    "self_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735618be",
   "metadata": {},
   "outputs": [],
   "source": [
    "self_sample_rate = cfg.DATA.SAMPLING_RATE\n",
    "self_sample_rate  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a30f8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "self_video_length = cfg.DATA.NUM_FRAMES\n",
    "self_video_length   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387446ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "self_seq_len = self_video_length * self_sample_rate\n",
    "self_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1455a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "self_num_classes = cfg.MODEL.STM.ACTION_CLASSES\n",
    "self_num_classes       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb3587e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmentation params.\n",
    "self_data_mean = cfg.DATA.MEAN\n",
    "self_data_std = cfg.DATA.STD\n",
    "self_use_bgr = cfg.AVA.BGR\n",
    "\n",
    "self_data_mean, self_data_std, self_use_bgr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b31b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "if self_split == \"train\":\n",
    "    self_jitter_min_scale = cfg.DATA.TRAIN_MIN_SCALES # list\n",
    "    self_jitter_max_scale = cfg.DATA.TRAIN_MAX_SCALE # int\n",
    "    self_random_horizontal_flip = cfg.DATA.RANDOM_FLIP\n",
    "    self_use_color_augmentation = cfg.AVA.TRAIN_USE_COLOR_AUGMENTATION\n",
    "    self_pca_jitter_only = cfg.AVA.TRAIN_PCA_JITTER_ONLY\n",
    "    self_pca_eigval = cfg.AVA.TRAIN_PCA_EIGVAL\n",
    "    self_pca_eigvec = cfg.AVA.TRAIN_PCA_EIGVEC\n",
    "else:\n",
    "    self_jitter_min_scale = cfg.DATA.TEST_MIN_SCALES\n",
    "    self_jitter_max_scale = cfg.DATA.TEST_MAX_SCALE\n",
    "    self_test_force_flip = cfg.AVA.TEST_FORCE_FLIP\n",
    "    \n",
    "    print(self_jitter_min_scale, self_jitter_max_scale, self_test_force_flip)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358c4252",
   "metadata": {},
   "source": [
    "##### 3.1.2 self._load_data(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825cb3b0",
   "metadata": {},
   "source": [
    " calls 'load_image_lists(cfg, is_train=(self._split == \"train\"))'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457f2d49",
   "metadata": {},
   "source": [
    "##### 3.1.2.1. Loading frame paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2dba31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from alphaction.dataset.datasets.ava_helper import load_image_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c045e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "(self_image_paths, self_video_idx_to_name) = load_image_lists(cfg, is_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247f0875",
   "metadata": {},
   "outputs": [],
   "source": [
    "self_image_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9a7baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "self_video_idx_to_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178c7f36",
   "metadata": {},
   "source": [
    "##### 3.1.2.2. Loading annotations for boxes and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6948900",
   "metadata": {},
   "outputs": [],
   "source": [
    "from alphaction.dataset.datasets.ava_helper import load_boxes_and_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47b622b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading annotations for boxes and labels.\n",
    "boxes_and_labels = load_boxes_and_labels(cfg, mode=self_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f92f5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes_and_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b697bcf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(boxes_and_labels) == len(self_image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31672cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes_and_labels = [\n",
    "            boxes_and_labels[self_video_idx_to_name[i]]\n",
    "            for i in range(len(self_image_paths))\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ba66a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes_and_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d735446",
   "metadata": {},
   "source": [
    "##### 3.1.2.3. Get indices of keyframes and corresponding boxes and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bea587",
   "metadata": {},
   "outputs": [],
   "source": [
    "from alphaction.dataset.datasets.ava_helper import get_keyframe_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb679e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get indices of keyframes and corresponding boxes and labels.\n",
    "(self_keyframe_indices, self_keyframe_boxes_and_labels,) = get_keyframe_data(boxes_and_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640533d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "self_keyframe_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0048e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "self_keyframe_boxes_and_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ca9acc",
   "metadata": {},
   "source": [
    "##### 3.1.2.4. Calculate the number of used boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f642c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from alphaction.dataset.datasets.ava_helper import get_num_boxes_used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85938d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "self_num_boxes_used = get_num_boxes_used(self_keyframe_indices, self_keyframe_boxes_and_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76276336",
   "metadata": {},
   "outputs": [],
   "source": [
    "self_num_boxes_used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3ac466",
   "metadata": {},
   "source": [
    "##### 3.1.3. setting `self.eval_file_paths`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef651b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "anno_dir = os.path.join(cfg.DATA.PATH_TO_DATA_DIR, cfg.AVA.ANNOTATION_DIR)\n",
    "csv_gt_file = os.path.join(anno_dir, cfg.AVA.TEST_GT_BOX_LISTS[0])\n",
    "labelmap_file = os.path.join(anno_dir, cfg.AVA.LABEL_MAP_FILE)\n",
    "exclusion_file = os.path.join(anno_dir, cfg.AVA.EXCLUSION_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fcf6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "anno_dir, csv_gt_file, labelmap_file, exclusion_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40125dd4",
   "metadata": {},
   "source": [
    "### 3.2. `__getitem__(self, idx)`\n",
    "\n",
    "Generate corresponding clips, boxes, labels and metadata for given idx.\n",
    "   * Args:`idx` (int): the video index provided by the pytorch sampler.\n",
    "   * Returns:\n",
    "       * **frames** (tensor): the frames of sampled from the video. The dimension is `channel` x `num frames` x `height` x `width`.\n",
    "       * **label** (ndarray): the label for correspond boxes for the current video.\n",
    "       * **time index** (zero): The time index is currently **not supported for AVA**.\n",
    "       * **idx** (int): the video index provided by the pytorch sampler.\n",
    "       * **extra_data** (dict): a dict containing extra data fields, like \"boxes\", \"ori_boxes\" and \"metadata\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d65efaf",
   "metadata": {},
   "source": [
    "#### def __getitem__(self, idx):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04db034d",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e931095",
   "metadata": {},
   "source": [
    "#### 3.2.1. Get the frame idxs for current clip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d7a540",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_idx, sec_idx, sec, center_idx = self_keyframe_indices[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096085c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_idx, sec_idx, sec, center_idx "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8dfb660",
   "metadata": {},
   "outputs": [],
   "source": [
    "from alphaction.dataset.datasets.utils import get_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5209bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = get_sequence(\n",
    "            center_idx,\n",
    "            self_seq_len // 2,\n",
    "            self_sample_rate,\n",
    "            num_frames=len(self_image_paths[video_idx]),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0775a166",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq # note that center_idx is located in 8th position of this seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25771b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_label_list = self_keyframe_boxes_and_labels[video_idx][sec_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89b29f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1299cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(clip_label_list) > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e31550e",
   "metadata": {},
   "source": [
    "#### 3.2.2. Get boxes and labels for current clip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd18026",
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes = []\n",
    "labels = []\n",
    "for box_labels in clip_label_list:\n",
    "    boxes.append(box_labels[0])\n",
    "    labels.append(box_labels[1])\n",
    "\n",
    "boxes, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9d224a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "boxes = np.array(boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42b41aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score is not used.\n",
    "boxes = boxes[:, :4].copy()\n",
    "# ori_boxes = boxes.copy()\n",
    "boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f125131",
   "metadata": {},
   "source": [
    "#### 3.2.3. Load images of current clip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26b3e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images of current clip.\n",
    "image_paths = [self_image_paths[video_idx][frame] for frame in seq]\n",
    "image_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a373e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from alphaction.dataset.datasets.utils import retry_load_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9cd550",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = retry_load_images(image_paths, backend='cv2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb98c860",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(imgs), imgs[0].shape # (16, (360, 486, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9119ece3",
   "metadata": {},
   "source": [
    "#### 3.2.4. Pre-processing of images and bboxes of current clip."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cf2586",
   "metadata": {},
   "source": [
    "In this step, we call `imgs, boxes = self._images_and_boxes_preprocessing_cv2(imgs, boxes=boxes)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4797a9ea",
   "metadata": {},
   "source": [
    "#### Inside of `self._images_and_boxes_preprocessing_cv2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4be3e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "height, width, _ = imgs[0].shape\n",
    "\n",
    "height, width # (360, 486)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac672ca",
   "metadata": {},
   "source": [
    "##### 3.2.4.1. undo the normalization of annotated bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d209506",
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes[:, [0, 2]] *= width\n",
    "boxes[:, [1, 3]] *= height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535f8512",
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060df00a",
   "metadata": {},
   "source": [
    "##### 3.2.4.2. applying transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ee2d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from alphaction.dataset.datasets import cv2_transform as cv2_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266b17a8",
   "metadata": {},
   "source": [
    "Clip the boxes with the height and width of the image size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b755295a",
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes = cv2_transform.clip_boxes_to_image(boxes, height, width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abab78ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea549e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# `transform.py` is list of np.array. However, for AVA, we only have\n",
    "# one np.array.\n",
    "boxes = [boxes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf840aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee18fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    " self_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02a78c5",
   "metadata": {},
   "source": [
    "Perform a spatial short scale jittering on the given images and corresponding boxes.\n",
    "\n",
    "* Args:\n",
    "   * images (list): list of images to perform scale jitter. Dimension is `height` x `width` x `channel`.\n",
    "   * min_size (int): the minimal size to scale the frames.\n",
    "   * max_size (int): the maximal size to scale the frames.\n",
    "   * boxes (list): optional. Corresponding boxes to images. Dimension is `num boxes` x 4.\n",
    "* Returns:\n",
    "    * (list): the list of scaled images with dimension of `new height` x `new width` x `channel`.\n",
    "    * (ndarray or None): the scaled boxes with dimension of `num boxes` x 4.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd864b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs, boxes = cv2_transform.random_short_side_scale_jitter(\n",
    "    imgs,\n",
    "    min_sizes=self_jitter_min_scale,\n",
    "    max_size=self_jitter_max_scale,\n",
    "    boxes=boxes,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8460d14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs[0].shape # (256, 346, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e16263b",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(boxes), boxes[0].shape, boxes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a54cf7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "self_test_force_flip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35e4b9c",
   "metadata": {},
   "source": [
    "##### 3.2.4.3. Convert image to CHW keeping BGR order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1db603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert image to CHW keeping BGR order.\n",
    "imgs = [cv2_transform.HWC2CHW(img) for img in imgs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7173ab65",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs[0].shape # (3, 256, 346)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c533e3",
   "metadata": {},
   "source": [
    "##### 3.2.4.4. color normalization of imgs to [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3287f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image [0, 255] -> [0, 1].\n",
    "imgs = [img / 255.0 for img in imgs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27bf359",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = [np.ascontiguousarray(\n",
    "    # img.reshape((3, self._crop_size, self._crop_size))\n",
    "    img.reshape((3, imgs[0].shape[1], imgs[0].shape[2]))\n",
    ").astype(np.float32) for img in imgs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea4c0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs[0].shape # (3, 256, 346)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7a2dfc",
   "metadata": {},
   "source": [
    "##### 3.2.4.5. Do color augmentation (after divided by 255.0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be8f00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "self_split == \"train\" and self_use_color_augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a10a0d",
   "metadata": {},
   "source": [
    "##### 3.2.4.6. Normalize images by mean and std."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e678080a",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = [\n",
    "            cv2_transform.color_normalization(\n",
    "                img,\n",
    "                np.array(self_data_mean, dtype=np.float32),\n",
    "                np.array(self_data_std, dtype=np.float32),\n",
    "            )\n",
    "            for img in imgs\n",
    "        ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a764a999",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs[0].shape, imgs[0][:,0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e340695",
   "metadata": {},
   "source": [
    "##### 3.2.4.6 Concat list of images to single ndarray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb39640",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = np.concatenate(\n",
    "            [np.expand_dims(img, axis=1) for img in imgs], axis=1\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadce734",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs.shape # (3, 16, 256, 346)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670bdc97",
   "metadata": {},
   "source": [
    "##### 3.2.4.7 Convert image format from BGR to RGB.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81511a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "self_use_bgr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d69523",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not self_use_bgr:\n",
    "    # Convert image format from BGR to RGB.\n",
    "    imgs = imgs[::-1, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8243361f",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs.shape # (3, 16, 256, 346)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff76d148",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = np.ascontiguousarray(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106bc6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs.shape # (3, 16, 256, 346)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c039617",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = torch.from_numpy(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f5625b",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs.shape # torch.Size([3, 16, 256, 346])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0afcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes = cv2_transform.clip_boxes_to_image(\n",
    "            boxes[0], imgs[0].shape[1], imgs[0].shape[2]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2425df33",
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes.shape, type(boxes) #((11, 4), numpy.ndarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ee1ff6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a85aca19",
   "metadata": {},
   "source": [
    "#### 3.2.5 Construct label arrays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badfaad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_arrs = np.zeros((len(labels), self_num_classes), dtype=np.int32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b2b153",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_arrs.shape # (11, 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b53cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels # [[80], [9], [9], [9], [80, 17, 12], [-1], [80, 9], [-1], [-1], [80, 9], [-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449fb08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, box_labels in enumerate(labels):\n",
    "    # AVA label index starts from 1.\n",
    "    for label in box_labels:\n",
    "        if label == -1:\n",
    "            continue\n",
    "        assert label >= 1 and label <= 80\n",
    "        label_arrs[i][label - 1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44c04eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_arrs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4e4bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pathways = cfg.MODEL.BACKBONE.PATHWAYS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b7b8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pathways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84173a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from alphaction.dataset.datasets.utils import pack_pathway_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1231ded3",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = pack_pathway_output(self_cfg, imgs, pathways=pathways)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe132b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(imgs), len(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f795ce0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs[0].shape # torch.Size([3, 16, 256, 346])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29323dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if pathways == 1:\n",
    "    slow, fast = imgs[0], None\n",
    "else:\n",
    "    slow, fast = imgs[0], imgs[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a683d82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "slow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866a7656",
   "metadata": {},
   "outputs": [],
   "source": [
    "fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212bf666",
   "metadata": {},
   "outputs": [],
   "source": [
    "h, w = slow.shape[-2:]\n",
    "h, w #(256, 346)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38075bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "whwh = torch.tensor([w, h, w, h], dtype=torch.float32)\n",
    "# tensor([346., 256., 346., 256.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd12251",
   "metadata": {},
   "outputs": [],
   "source": [
    "whwh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ff3543",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = [video_idx, sec]\n",
    "metadata # [0, 902]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1eb4e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089b3547",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743c33c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "10dbf8fa",
   "metadata": {},
   "source": [
    "### COMPARISON OF OUTPUT OF DATALOADER AND DATASET\n",
    "\n",
    "* We notice that the shape of output of dataloder should be always divisible to `cfg.DATALOADER.SIZE_DIVISIBILITY`.\n",
    "* Padding is done in `BatchCollator`, e.g., ` batch_different_videos` defined in [HERE](https://github.com/MCG-NJU/STMixer/blob/main/alphaction/dataset/collate_batch.py).\n",
    "* Padding seems to be one-sided."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca979c1",
   "metadata": {},
   "source": [
    "output of dataloader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a9685b",
   "metadata": {},
   "outputs": [],
   "source": [
    "slow_clips.shape, slow_clips.dtype, slow_clips.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38a1db7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac2ffec",
   "metadata": {},
   "outputs": [],
   "source": [
    "slow.shape, slow.dtype, slow.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a9bfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "slow_clips_without_batch = slow_clips[0,:,:,:,:346]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aade50bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "slow_clips_without_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2d68e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.equal(slow_clips_without_batch, slow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240a64da",
   "metadata": {},
   "outputs": [],
   "source": [
    "slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22abd999",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "744e5c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 16, 256, 346])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "def batch_different_videos(videos, size_divisible=0):\n",
    "    '''\n",
    "    :param videos: a list of video tensors\n",
    "    :param size_divisible: output_size(width and height) should be divisble by this param\n",
    "    :return: batched videos as a single tensor\n",
    "    '''\n",
    "    assert isinstance(videos, (tuple, list))\n",
    "    max_size = tuple(max(s) for s in zip(*[clip.shape for clip in videos]))\n",
    "    \n",
    "    # max_size: (3, 16, 256, 346)\n",
    "\n",
    "    if size_divisible > 0:\n",
    "        stride = size_divisible\n",
    "        max_size = list(max_size)\n",
    "        max_size[2] = int(math.ceil(max_size[2] / stride) * stride)\n",
    "        max_size[3] = int(math.ceil(max_size[3] / stride) * stride)\n",
    "        max_size = tuple(max_size)\n",
    "    \n",
    "    # max_size: (3, 16, 256, 352)\n",
    "    \n",
    "    batch_shape = (len(videos),) + max_size # (1, 3, 16, 256, 352)\n",
    "    \n",
    "    batched_clips = videos[0].new(*batch_shape).zero_()\n",
    "    for clip, pad_clip in zip(videos, batched_clips):\n",
    "        # clip.shape: torch.Size([3, 16, 256, 346])\n",
    "        pad_clip[:clip.shape[0], :clip.shape[1], :clip.shape[2], :clip.shape[3]].copy_(clip)\n",
    "\n",
    "    return batched_clips\n",
    "\n",
    "# Example Usage:\n",
    "input_tensor = [torch.rand(3, 16, 256, 346)]  # Example tensor of shape 3x16x128x256\n",
    "size_divisible = 32  # Example size divisible\n",
    "batched_result = batch_different_videos(input_tensor, size_divisible)\n",
    "#print(batched_result.shape)  # Output shape of the batched tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae03536",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa982a52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3142931f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c6fd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_lists = cfg.AVA.TEST_GT_BOX_LISTS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3195f135",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_filenames = [\n",
    "        os.path.join(cfg.DATA.PATH_TO_DATA_DIR, cfg.AVA.ANNOTATION_DIR, filename)\n",
    "        for filename in gt_lists\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72435bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c2d99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_is_gt_box = [True] * len(gt_lists)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63875cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_is_gt_box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba558ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from alphaction.dataset.datasets.ava_helper import parse_bboxes_file, load_boxes_and_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc191e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes_and_labels = load_boxes_and_labels(cfg, mode='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdafdad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from alphaction.dataset.datasets.ava_helper import load_image_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d90ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "(image_paths, video_idx_to_name) = load_image_lists(cfg, is_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002b0340",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea66b089",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_idx_to_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d78f8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes_and_labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a754007",
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes_and_labels = [\n",
    "            boxes_and_labels[video_idx_to_name[i]]\n",
    "            for i in range(len(image_paths))\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea690972",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_idx_to_name = ['-5KQ66BBWC4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5f8f2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9d2e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_filenames = [\n",
    "        os.path.join(cfg.DATA.PATH_TO_DATA_DIR, cfg.AVA.FRAME_LIST_DIR, filename)\n",
    "        for filename in (\n",
    "            cfg.AVA.TRAIN_LISTS if False else cfg.AVA.TEST_LISTS\n",
    "        )\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f08b329",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75757d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from iopath.common.file_io import g_pathmgr as pathmgr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39ace68",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = defaultdict(list)\n",
    "video_name_to_idx = {}\n",
    "video_idx_to_name = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e16d942",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for list_filename in list_filenames:\n",
    "    with pathmgr.open(list_filename, \"r\") as f:\n",
    "        f.readline()\n",
    "        for line in f:\n",
    "            row = line.split()\n",
    "            # The format of each row should follow:\n",
    "            # original_vido_id video_id frame_id path labels.\n",
    "            assert len(row) == 5\n",
    "            video_name = row[0]\n",
    "\n",
    "            if video_name not in video_name_to_idx:\n",
    "                idx = len(video_name_to_idx)\n",
    "                video_name_to_idx[video_name] = idx\n",
    "                video_idx_to_name.append(video_name)\n",
    "\n",
    "            data_key = video_name_to_idx[video_name]\n",
    "\n",
    "            image_paths[data_key].append(\n",
    "                    os.path.join(cfg.DATA.PATH_TO_DATA_DIR, cfg.AVA.FRAME_DIR, row[3])\n",
    "                )\n",
    "\n",
    "image_paths = [image_paths[i] for i in range(len(image_paths))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb4d087",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_idx_to_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b7643e",
   "metadata": {},
   "outputs": [],
   "source": [
    "row[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b6bd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb16c8b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42eb0d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856e5da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.DATALOADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91cbb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79216fb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2195d93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93c80f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader_test = data_loaders_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44fb38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(data_loader_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20c7dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = iter(data_loader_test)\n",
    "one_item = next(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cd9066",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576e63a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = one_item[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97c5188",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a40ab79",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_item[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780fc556",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
