{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74dd3622",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import cv2\n",
    "import math\n",
    "import numpy as np\n",
    "import moviepy \n",
    "import torch\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "from my_alphaction.config import cfg\n",
    "from my_alphaction.modeling.detector import build_detection_model\n",
    "from my_alphaction.utils.checkpoint import ActionCheckpointer\n",
    "from my_alphaction.utils.comm import get_world_size\n",
    "\n",
    "\n",
    "from my_utils.gen_utils import create_experiment_folder\n",
    "from my_utils.video_processing import get_video_info, get_frame_from_video\n",
    "from my_utils.slicing import get_slice_bboxes, generate_sliding_window_gif\n",
    "\n",
    "from my_utils.video_processing import segment_crop_video\n",
    "from my_utils.my_ava_preprocessing import ava_preprocessing_cv2, clip_constructor, prepare_collated_batches\n",
    "from my_utils.ava_postprocessing import concatenate_results\n",
    "from my_utils.visualization import action_visualizer_frame_index\n",
    "\n",
    "from my_utils.gen_utils import parse_label_file\n",
    "\n",
    "from my_utils.ava_postprocessing import clip_boxes_tensor, map_bbox_from_prep_to_crop, map_bbox_from_crop_to_orig\n",
    "\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b7ce27",
   "metadata": {},
   "source": [
    "### 1. CONFIG\n",
    "#### 1.1 Main Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6766963",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'VMAEv2'\n",
    "\n",
    "\n",
    "person_threshold = 0.6 # confidence threshold on actor. 0.6 is the defualt\n",
    "sampling_rate = 3 # sampling rate: 4 is the defualt\n",
    "top_k = 5 # number of actions per person\n",
    "video_path = '../input_dir/markt2_fight.mp4'\n",
    "\n",
    "slice_height = 800\n",
    "slice_width = 1000\n",
    "overlap_ratio = 0.1\n",
    "\n",
    "starting_frame_index = 100\n",
    "length_input = 200\n",
    "\n",
    "exp_dict = {'model_name': model_name,\n",
    "            'model_params': {'person_threshold': person_threshold, \n",
    "                             'sampling_rate': sampling_rate},\n",
    "            'orig_post_processing':{'top_k': top_k},\n",
    "            'aggregation': {'method': {}, \n",
    "                            'params': {}},\n",
    "            'video_path': video_path,\n",
    "            'slicing_params': {'slice_height': slice_height, \n",
    "                               'slice_width': slice_width, \n",
    "                               'overlap_ratio':overlap_ratio},\n",
    "            'video_params': {'st_frame_index': starting_frame_index, \n",
    "                             'length_input':length_input\n",
    "                             }\n",
    "           }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13831d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_name == 'VMAEv2':\n",
    "    config_file = '../config_files/VMAEv2-ViTB-16x4.yaml'\n",
    "if model_name == 'VMAE':\n",
    "    config_file = '../config_files/VMAE-ViTB-16x4.yaml'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f887add7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.merge_from_file(config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "310ceda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change model weight path\n",
    "if model_name == 'VMAEv2':\n",
    "    cfg.merge_from_list([\"MODEL.WEIGHT\", \"../checkpoints/VMAEv2_ViTB_16x4.pth\"])\n",
    "if model_name == 'VMAE':\n",
    "    cfg.merge_from_list([\"MODEL.WEIGHT\", \"../checkpoints/VMAE_ViTB_16x4.pth\"])\n",
    "\n",
    "# change output dir\n",
    "cfg.merge_from_list([\"OUTPUT_DIR\", \"../output_dir/\"])\n",
    "\n",
    "# change person threshold\n",
    "cfg.merge_from_list([\"MODEL.STM.PERSON_THRESHOLD\", person_threshold])\n",
    "\n",
    "# change sampling rate\n",
    "cfg.merge_from_list([\"DATA.SAMPLING_RATE\", sampling_rate])\n",
    "\n",
    "# change path for data_dir\n",
    "cfg.merge_from_list([\"DATA.PATH_TO_DATA_DIR\", \"/work/ava\"])\n",
    "\n",
    "# folder name of annotations\n",
    "cfg.merge_from_list([\"AVA.ANNOTATION_DIR\", \"annotations/\"])\n",
    "\n",
    "# file name of  frame_lists\n",
    "cfg.merge_from_list([\"AVA.TRAIN_LISTS\", ['sample.csv']])\n",
    "cfg.merge_from_list([\"AVA.TEST_LISTS\", ['sample.csv']])\n",
    "\n",
    "# file name of predicted_bboxes\n",
    "cfg.merge_from_list([\"AVA.TRAIN_GT_BOX_LISTS\", ['ava_sample_predicted_boxes.csv']])\n",
    "cfg.merge_from_list([\"AVA.TEST_GT_BOX_LISTS\", ['ava_sample_predicted_boxes.csv']])\n",
    "\n",
    "# file name of exlusions\n",
    "cfg.merge_from_list([\"AVA.EXCLUSION_FILE\", 'ava_sample_train_excluded_timestamps_v2.2.csv'])\n",
    "\n",
    "# number of batches in test scenario\n",
    "cfg.merge_from_list([\"TEST.VIDEOS_PER_BATCH\", 1])\n",
    "\n",
    "# number of workers\n",
    "cfg.merge_from_list([\"DATALOADER.NUM_WORKERS\", 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25e23e86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.ViT.USE_CHECKPOINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c0319f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.merge_from_list([\"ViT.USE_CHECKPOINT\", False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81d908cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.ViT.USE_CHECKPOINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33b27cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cfg.DATALOADER.SIZE_DIVISIBILITY:  32\n",
      "cfg.DATA.SAMPLING_RATE:  3\n",
      "cfg.DATA.NUM_FRAMES:  16\n",
      "self_seq_len:  48\n",
      "cfg.MODEL.STM.ACTION_CLASSES:  80\n",
      "Augmentation params:  [0.45, 0.45, 0.45] [0.225, 0.225, 0.225] False\n",
      "scale and flip params [256] 1333 False\n"
     ]
    }
   ],
   "source": [
    "debug = True\n",
    "if debug:\n",
    "    # The shape of model input should be divisible into this. Otherwise, padding 0 to left and bottum. \n",
    "    print(\"cfg.DATALOADER.SIZE_DIVISIBILITY: \", cfg.DATALOADER.SIZE_DIVISIBILITY)\n",
    "    \n",
    "    # Sampling rate in constructing the clips.\n",
    "    self_sample_rate =  cfg.DATA.SAMPLING_RATE\n",
    "    print(\"cfg.DATA.SAMPLING_RATE: \", cfg.DATA.SAMPLING_RATE)\n",
    "    \n",
    "    # Length of clip\n",
    "    self_video_length = cfg.DATA.NUM_FRAMES\n",
    "    print(\"cfg.DATA.NUM_FRAMES: \", cfg.DATA.NUM_FRAMES)\n",
    "    \n",
    "    # Length of sequence frames from which a clip is constructed.\n",
    "    self_seq_len = self_video_length * self_sample_rate\n",
    "    print(\"self_seq_len: \", self_seq_len)\n",
    "    \n",
    "    self_num_classes = cfg.MODEL.STM.ACTION_CLASSES\n",
    "    print(\"cfg.MODEL.STM.ACTION_CLASSES: \", self_num_classes)\n",
    "    \n",
    "    # Augmentation params.\n",
    "    self_data_mean = cfg.DATA.MEAN\n",
    "    self_data_std = cfg.DATA.STD\n",
    "    self_use_bgr = cfg.AVA.BGR\n",
    "    print(\"Augmentation params: \", self_data_mean, self_data_std, self_use_bgr)\n",
    "    \n",
    "    self_jitter_min_scale = cfg.DATA.TEST_MIN_SCALES\n",
    "    self_jitter_max_scale = cfg.DATA.TEST_MAX_SCALE\n",
    "    self_test_force_flip = cfg.AVA.TEST_FORCE_FLIP\n",
    "\n",
    "    print(\"scale and flip params\", self_jitter_min_scale, self_jitter_max_scale, self_test_force_flip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5baed4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_detection_model(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "895c6824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpointer = ActionCheckpointer(cfg, model, save_dir=\"../output_dir/\")\n",
    "checkpointer.load(cfg.MODEL.WEIGHT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50383232",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    backbone_module = model._modules['backbone']\n",
    "    backbone_module = backbone_module.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77de51af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.onnx\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a64e78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c199bd3",
   "metadata": {},
   "outputs": [
    {
     "ename": "SymbolicValueError",
     "evalue": "Failed to export a node '%2645 : Long(device=cpu) = onnx::Squeeze(%2643, %2644), scope: my_alphaction.modeling.detector.stm_detector.STMDetector::/my_alphaction.modeling.stm_decoder.stm_decoder.STMDecoder::stm_head/my_alphaction.modeling.stm_decoder.stm_decoder.AMStage::decoder_stages.0/my_alphaction.modeling.stm_decoder.stm_decoder.AdaptiveSTSamplingMixing::samplingmixing/my_alphaction.modeling.stm_decoder.util.adaptive_mixing_operator.AdaptiveMixing::adaptive_mixing_s # /work/my_alphaction/modeling/stm_decoder/util/adaptive_mixing_operator.py:78:0\n' (in list node %2654 : int[] = prim::ListConstruct(%2645, %2653), scope: my_alphaction.modeling.detector.stm_detector.STMDetector::/my_alphaction.modeling.stm_decoder.stm_decoder.STMDecoder::stm_head/my_alphaction.modeling.stm_decoder.stm_decoder.AMStage::decoder_stages.0/my_alphaction.modeling.stm_decoder.stm_decoder.AdaptiveSTSamplingMixing::samplingmixing/my_alphaction.modeling.stm_decoder.util.adaptive_mixing_operator.AdaptiveMixing::adaptive_mixing_s\n) because it is not constant. Please try to make things (e.g. kernel sizes) static if possible.  [Caused by the value '2654 defined in (%2654 : int[] = prim::ListConstruct(%2645, %2653), scope: my_alphaction.modeling.detector.stm_detector.STMDetector::/my_alphaction.modeling.stm_decoder.stm_decoder.STMDecoder::stm_head/my_alphaction.modeling.stm_decoder.stm_decoder.AMStage::decoder_stages.0/my_alphaction.modeling.stm_decoder.stm_decoder.AdaptiveSTSamplingMixing::samplingmixing/my_alphaction.modeling.stm_decoder.util.adaptive_mixing_operator.AdaptiveMixing::adaptive_mixing_s\n)' (type 'List[int]') in the TorchScript graph. The containing node has kind 'prim::ListConstruct'.] \n\n    Inputs:\n        #0: 2645 defined in (%2645 : Long(device=cpu) = onnx::Squeeze(%2643, %2644), scope: my_alphaction.modeling.detector.stm_detector.STMDetector::/my_alphaction.modeling.stm_decoder.stm_decoder.STMDecoder::stm_head/my_alphaction.modeling.stm_decoder.stm_decoder.AMStage::decoder_stages.0/my_alphaction.modeling.stm_decoder.stm_decoder.AdaptiveSTSamplingMixing::samplingmixing/my_alphaction.modeling.stm_decoder.util.adaptive_mixing_operator.AdaptiveMixing::adaptive_mixing_s # /work/my_alphaction/modeling/stm_decoder/util/adaptive_mixing_operator.py:78:0\n    )  (type 'Tensor')\n        #1: 2653 defined in (%2653 : Long(device=cpu) = onnx::Squeeze(%2651, %2652), scope: my_alphaction.modeling.detector.stm_detector.STMDetector::/my_alphaction.modeling.stm_decoder.stm_decoder.STMDecoder::stm_head/my_alphaction.modeling.stm_decoder.stm_decoder.AMStage::decoder_stages.0/my_alphaction.modeling.stm_decoder.stm_decoder.AdaptiveSTSamplingMixing::samplingmixing/my_alphaction.modeling.stm_decoder.util.adaptive_mixing_operator.AdaptiveMixing::adaptive_mixing_s # /work/my_alphaction/modeling/stm_decoder/util/adaptive_mixing_operator.py:78:0\n    )  (type 'Tensor')\n    Outputs:\n        #0: 2654 defined in (%2654 : int[] = prim::ListConstruct(%2645, %2653), scope: my_alphaction.modeling.detector.stm_detector.STMDetector::/my_alphaction.modeling.stm_decoder.stm_decoder.STMDecoder::stm_head/my_alphaction.modeling.stm_decoder.stm_decoder.AMStage::decoder_stages.0/my_alphaction.modeling.stm_decoder.stm_decoder.AdaptiveSTSamplingMixing::samplingmixing/my_alphaction.modeling.stm_decoder.util.adaptive_mixing_operator.AdaptiveMixing::adaptive_mixing_s\n    )  (type 'List[int]')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSymbolicValueError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39meval()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Export the model to ONNX with static batch size\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43monnx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexport\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m                  \u001b[49m\u001b[43margs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mslow_video\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwhole_model_dyn_batch.onnx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m                  \u001b[49m\u001b[43minput_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mslow_video\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m                  \u001b[49m\u001b[43moutput_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mopset_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m17\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m     17\u001b[0m \u001b[43m                  \u001b[49m\u001b[38;5;66;43;03m#dynamic_axes={\"slow_video\": {0: \"batch_size\"}, \"whwh\": {0: \"batch_size\"}, \"output\": {0: \"batch_size\"}}\u001b[39;49;00m\n\u001b[1;32m     18\u001b[0m \u001b[43m                 \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/onnx/utils.py:504\u001b[0m, in \u001b[0;36mexport\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, custom_opsets, export_modules_as_functions)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;129m@_beartype\u001b[39m\u001b[38;5;241m.\u001b[39mbeartype\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexport\u001b[39m(\n\u001b[1;32m    188\u001b[0m     model: Union[torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule, torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mScriptModule, torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mScriptFunction],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    204\u001b[0m     export_modules_as_functions: Union[\u001b[38;5;28mbool\u001b[39m, Collection[Type[torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    205\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Exports a model into ONNX format.\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \n\u001b[1;32m    208\u001b[0m \u001b[38;5;124;03m    If ``model`` is not a :class:`torch.jit.ScriptModule` nor a\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    501\u001b[0m \u001b[38;5;124;03m            All errors are subclasses of :class:`errors.OnnxExporterError`.\u001b[39;00m\n\u001b[1;32m    502\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 504\u001b[0m     \u001b[43m_export\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    506\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[43m        \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexport_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    511\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    512\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    513\u001b[0m \u001b[43m        \u001b[49m\u001b[43moperator_export_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moperator_export_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    514\u001b[0m \u001b[43m        \u001b[49m\u001b[43mopset_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mopset_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_constant_folding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_constant_folding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdynamic_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_initializers_as_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_initializers_as_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_opsets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_opsets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexport_modules_as_functions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexport_modules_as_functions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/onnx/utils.py:1529\u001b[0m, in \u001b[0;36m_export\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, fixed_batch_size, custom_opsets, add_node_names, onnx_shape_inference, export_modules_as_functions)\u001b[0m\n\u001b[1;32m   1526\u001b[0m     dynamic_axes \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m   1527\u001b[0m _validate_dynamic_axes(dynamic_axes, model, input_names, output_names)\n\u001b[0;32m-> 1529\u001b[0m graph, params_dict, torch_out \u001b[38;5;241m=\u001b[39m \u001b[43m_model_to_graph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1530\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1531\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1532\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1533\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1534\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1535\u001b[0m \u001b[43m    \u001b[49m\u001b[43moperator_export_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1536\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_do_constant_folding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1537\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfixed_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfixed_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1538\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1539\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdynamic_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1540\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1542\u001b[0m \u001b[38;5;66;03m# TODO: Don't allocate a in-memory string for the protobuf\u001b[39;00m\n\u001b[1;32m   1543\u001b[0m defer_weight_export \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1544\u001b[0m     export_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _exporter_states\u001b[38;5;241m.\u001b[39mExportTypes\u001b[38;5;241m.\u001b[39mPROTOBUF_FILE\n\u001b[1;32m   1545\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/onnx/utils.py:1115\u001b[0m, in \u001b[0;36m_model_to_graph\u001b[0;34m(model, args, verbose, input_names, output_names, operator_export_type, do_constant_folding, _disable_torch_constant_prop, fixed_batch_size, training, dynamic_axes)\u001b[0m\n\u001b[1;32m   1112\u001b[0m params_dict \u001b[38;5;241m=\u001b[39m _get_named_param_dict(graph, params)\n\u001b[1;32m   1114\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1115\u001b[0m     graph \u001b[38;5;241m=\u001b[39m \u001b[43m_optimize_graph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1117\u001b[0m \u001b[43m        \u001b[49m\u001b[43moperator_export_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1118\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_disable_torch_constant_prop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_disable_torch_constant_prop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1119\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfixed_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfixed_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1120\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdynamic_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1122\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1123\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodule\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1124\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1125\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1126\u001b[0m     torch\u001b[38;5;241m.\u001b[39monnx\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch IR graph at exception: \u001b[39m\u001b[38;5;124m\"\u001b[39m, graph)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/onnx/utils.py:663\u001b[0m, in \u001b[0;36m_optimize_graph\u001b[0;34m(graph, operator_export_type, _disable_torch_constant_prop, fixed_batch_size, params_dict, dynamic_axes, input_names, module)\u001b[0m\n\u001b[1;32m    660\u001b[0m     _C\u001b[38;5;241m.\u001b[39m_jit_pass_onnx_set_dynamic_input_shape(graph, dynamic_axes, input_names)\n\u001b[1;32m    661\u001b[0m _C\u001b[38;5;241m.\u001b[39m_jit_pass_onnx_lint(graph)\n\u001b[0;32m--> 663\u001b[0m graph \u001b[38;5;241m=\u001b[39m \u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jit_pass_onnx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator_export_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    664\u001b[0m _C\u001b[38;5;241m.\u001b[39m_jit_pass_onnx_lint(graph)\n\u001b[1;32m    665\u001b[0m _C\u001b[38;5;241m.\u001b[39m_jit_pass_lint(graph)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/onnx/utils.py:1888\u001b[0m, in \u001b[0;36m_run_symbolic_function\u001b[0;34m(graph, block, node, inputs, env, operator_export_type)\u001b[0m\n\u001b[1;32m   1883\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m symbolic_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1884\u001b[0m         \u001b[38;5;66;03m# TODO Wrap almost identical attrs assignment or comment the difference.\u001b[39;00m\n\u001b[1;32m   1885\u001b[0m         attrs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   1886\u001b[0m             k: symbolic_helper\u001b[38;5;241m.\u001b[39m_node_get(node, k) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m node\u001b[38;5;241m.\u001b[39mattributeNames()\n\u001b[1;32m   1887\u001b[0m         }\n\u001b[0;32m-> 1888\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msymbolic_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m attrs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   1891\u001b[0m     k \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m node\u001b[38;5;241m.\u001b[39mkindOf(k)[\u001b[38;5;241m0\u001b[39m]: symbolic_helper\u001b[38;5;241m.\u001b[39m_node_get(node, k)\n\u001b[1;32m   1892\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m node\u001b[38;5;241m.\u001b[39mattributeNames()\n\u001b[1;32m   1893\u001b[0m }\n\u001b[1;32m   1894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m namespace \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124monnx\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1895\u001b[0m     \u001b[38;5;66;03m# Clone node to trigger ONNX shape inference\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/onnx/symbolic_helper.py:286\u001b[0m, in \u001b[0;36mparse_args.<locals>.decorator.<locals>.wrapper\u001b[0;34m(g, *args, **kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     arg_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(args)  \u001b[38;5;66;03m# type: ignore[list-item]\u001b[39;00m\n\u001b[1;32m    285\u001b[0m     fn_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 286\u001b[0m args \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    287\u001b[0m     _parse_arg(arg, arg_desc, arg_name, fn_name)  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m arg, arg_desc, arg_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(args, arg_descriptors, arg_names)\n\u001b[1;32m    289\u001b[0m ]\n\u001b[1;32m    290\u001b[0m \u001b[38;5;66;03m# only support _outputs in kwargs\u001b[39;00m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m, (\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSymbolic function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m**kwargs\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m can contain a single \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkey/value entry. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    294\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mFILE_BUG_MSG\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    295\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/onnx/symbolic_helper.py:287\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    284\u001b[0m     arg_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(args)  \u001b[38;5;66;03m# type: ignore[list-item]\u001b[39;00m\n\u001b[1;32m    285\u001b[0m     fn_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    286\u001b[0m args \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 287\u001b[0m     \u001b[43m_parse_arg\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg_desc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn_name\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m arg, arg_desc, arg_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(args, arg_descriptors, arg_names)\n\u001b[1;32m    289\u001b[0m ]\n\u001b[1;32m    290\u001b[0m \u001b[38;5;66;03m# only support _outputs in kwargs\u001b[39;00m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m, (\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSymbolic function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m**kwargs\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m can contain a single \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkey/value entry. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    294\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mFILE_BUG_MSG\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    295\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/onnx/symbolic_helper.py:104\u001b[0m, in \u001b[0;36m_parse_arg\u001b[0;34m(value, desc, arg_name, node_name)\u001b[0m\n\u001b[1;32m    102\u001b[0m         element_node \u001b[38;5;241m=\u001b[39m v\u001b[38;5;241m.\u001b[39mnode()\n\u001b[1;32m    103\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m element_node\u001b[38;5;241m.\u001b[39mkind() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124monnx::Constant\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 104\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mSymbolicValueError(\n\u001b[1;32m    105\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to export a node \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00melement_node\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    106\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(in list node \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    107\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbecause it is not constant. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    108\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease try to make things (e.g. kernel sizes) static if possible.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    109\u001b[0m                 value,\n\u001b[1;32m    110\u001b[0m             )\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mint\u001b[39m(_node_get(v\u001b[38;5;241m.\u001b[39mnode(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m value\u001b[38;5;241m.\u001b[39mnode()\u001b[38;5;241m.\u001b[39minputs()]\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mSymbolicValueError\u001b[0m: Failed to export a node '%2645 : Long(device=cpu) = onnx::Squeeze(%2643, %2644), scope: my_alphaction.modeling.detector.stm_detector.STMDetector::/my_alphaction.modeling.stm_decoder.stm_decoder.STMDecoder::stm_head/my_alphaction.modeling.stm_decoder.stm_decoder.AMStage::decoder_stages.0/my_alphaction.modeling.stm_decoder.stm_decoder.AdaptiveSTSamplingMixing::samplingmixing/my_alphaction.modeling.stm_decoder.util.adaptive_mixing_operator.AdaptiveMixing::adaptive_mixing_s # /work/my_alphaction/modeling/stm_decoder/util/adaptive_mixing_operator.py:78:0\n' (in list node %2654 : int[] = prim::ListConstruct(%2645, %2653), scope: my_alphaction.modeling.detector.stm_detector.STMDetector::/my_alphaction.modeling.stm_decoder.stm_decoder.STMDecoder::stm_head/my_alphaction.modeling.stm_decoder.stm_decoder.AMStage::decoder_stages.0/my_alphaction.modeling.stm_decoder.stm_decoder.AdaptiveSTSamplingMixing::samplingmixing/my_alphaction.modeling.stm_decoder.util.adaptive_mixing_operator.AdaptiveMixing::adaptive_mixing_s\n) because it is not constant. Please try to make things (e.g. kernel sizes) static if possible.  [Caused by the value '2654 defined in (%2654 : int[] = prim::ListConstruct(%2645, %2653), scope: my_alphaction.modeling.detector.stm_detector.STMDetector::/my_alphaction.modeling.stm_decoder.stm_decoder.STMDecoder::stm_head/my_alphaction.modeling.stm_decoder.stm_decoder.AMStage::decoder_stages.0/my_alphaction.modeling.stm_decoder.stm_decoder.AdaptiveSTSamplingMixing::samplingmixing/my_alphaction.modeling.stm_decoder.util.adaptive_mixing_operator.AdaptiveMixing::adaptive_mixing_s\n)' (type 'List[int]') in the TorchScript graph. The containing node has kind 'prim::ListConstruct'.] \n\n    Inputs:\n        #0: 2645 defined in (%2645 : Long(device=cpu) = onnx::Squeeze(%2643, %2644), scope: my_alphaction.modeling.detector.stm_detector.STMDetector::/my_alphaction.modeling.stm_decoder.stm_decoder.STMDecoder::stm_head/my_alphaction.modeling.stm_decoder.stm_decoder.AMStage::decoder_stages.0/my_alphaction.modeling.stm_decoder.stm_decoder.AdaptiveSTSamplingMixing::samplingmixing/my_alphaction.modeling.stm_decoder.util.adaptive_mixing_operator.AdaptiveMixing::adaptive_mixing_s # /work/my_alphaction/modeling/stm_decoder/util/adaptive_mixing_operator.py:78:0\n    )  (type 'Tensor')\n        #1: 2653 defined in (%2653 : Long(device=cpu) = onnx::Squeeze(%2651, %2652), scope: my_alphaction.modeling.detector.stm_detector.STMDetector::/my_alphaction.modeling.stm_decoder.stm_decoder.STMDecoder::stm_head/my_alphaction.modeling.stm_decoder.stm_decoder.AMStage::decoder_stages.0/my_alphaction.modeling.stm_decoder.stm_decoder.AdaptiveSTSamplingMixing::samplingmixing/my_alphaction.modeling.stm_decoder.util.adaptive_mixing_operator.AdaptiveMixing::adaptive_mixing_s # /work/my_alphaction/modeling/stm_decoder/util/adaptive_mixing_operator.py:78:0\n    )  (type 'Tensor')\n    Outputs:\n        #0: 2654 defined in (%2654 : int[] = prim::ListConstruct(%2645, %2653), scope: my_alphaction.modeling.detector.stm_detector.STMDetector::/my_alphaction.modeling.stm_decoder.stm_decoder.STMDecoder::stm_head/my_alphaction.modeling.stm_decoder.stm_decoder.AMStage::decoder_stages.0/my_alphaction.modeling.stm_decoder.stm_decoder.AdaptiveSTSamplingMixing::samplingmixing/my_alphaction.modeling.stm_decoder.util.adaptive_mixing_operator.AdaptiveMixing::adaptive_mixing_s\n    )  (type 'List[int]')"
     ]
    }
   ],
   "source": [
    "slow_video = torch.randn(1, 3, 16, 256, 320).to('cuda')\n",
    "values = [[320., 256., 320., 256.]]\n",
    "\n",
    "# Create the tensor\n",
    "whwh = torch.tensor(values, device='cuda')\n",
    "\n",
    "model = model.eval().to('cuda')\n",
    "\n",
    "# Export the model to ONNX with static batch size\n",
    "torch.onnx.export(model, \n",
    "                  args = slow_video, \n",
    "                  f=\"whole_model_dyn_batch.onnx\", \n",
    "                  input_names=[\"slow_video\"], \n",
    "                  output_names=[\"output\"],\n",
    "                  opset_version=17,\n",
    "                  verbose=False\n",
    "                  #dynamic_axes={\"slow_video\": {0: \"batch_size\"}, \"whwh\": {0: \"batch_size\"}, \"output\": {0: \"batch_size\"}}\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93653dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "traced_model = torch.jit.trace(model, [slow_video])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3f5944",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.jit.optimized_execution(True):\n",
    "    scripted_model = torch.jit.script(model, [slow_video]).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10020fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.onnx.export(model, slow_video, \"model_v2.onnx\", do_constant_folding=False, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84836c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.onnx.export(traced_model, slow_video, \"traced_model.onnx\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aab42ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = onnx.load(\"your_model.onnx\")\n",
    "onnx.checker.check_model(my_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83c22bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# iterate through inputs of the graph\n",
    "for input in my_model.graph.input:\n",
    "    print (input.name, end=\": \")\n",
    "    # get type of input tensor\n",
    "    tensor_type = input.type.tensor_type\n",
    "    # check if it has a shape:\n",
    "    if (tensor_type.HasField(\"shape\")):\n",
    "        # iterate through dimensions of the shape:\n",
    "        for d in tensor_type.shape.dim:\n",
    "            # the dimension may have a definite (integer) value or a symbolic identifier or neither:\n",
    "            if (d.HasField(\"dim_value\")):\n",
    "                print (d.dim_value, end=\", \")  # known dimension\n",
    "            elif (d.HasField(\"dim_param\")):\n",
    "                print (d.dim_param, end=\", \")  # unknown dimension with symbolic name\n",
    "            else:\n",
    "                print (\"?\", end=\", \")  # unknown dimension with no name\n",
    "    else:\n",
    "        print (\"unknown rank\", end=\"\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47181c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract backbone and lateral_convs modules\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "backbone_module = model._modules['backbone']\n",
    "lateral_convs_module = model._modules['lateral_convs']\n",
    "\n",
    "# Assuming you want to combine them into a single model\n",
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self, backbone, lateral_convs):\n",
    "        super(CombinedModel, self).__init__()\n",
    "        self.backbone = backbone\n",
    "        self.lateral_convs = lateral_convs\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through the backbone and lateral_convs\n",
    "        backbone_output = self.backbone(x)\n",
    "        lateral_convs_output = self.lateral_convs(backbone_output)\n",
    "        return lateral_convs_output\n",
    "\n",
    "# Create the combined model instance\n",
    "combined_model = CombinedModel(backbone_module, lateral_convs_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f5f4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_input = torch.randn(1, 3, 16, 256, 320)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4be1da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_model([example_input])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdc2bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b7c28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "print(\"ONNX version:\", onnx.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed651488",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8716c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Sequence\n",
    "import numpy as np\n",
    "import onnx\n",
    "import onnxruntime\n",
    "import subprocess\n",
    "import sys\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "from typing import Any, Callable, Dict, List, Optional, Sequence, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import onnx\n",
    "from onnx.backend.test.case.test_case import TestCase\n",
    "from onnx.backend.test.case.utils import import_recursive\n",
    "from onnx.onnx_pb import (\n",
    "    AttributeProto,\n",
    "    FunctionProto,\n",
    "    GraphProto,\n",
    "    ModelProto,\n",
    "    NodeProto,\n",
    "    TensorProto,\n",
    "    TypeProto,\n",
    ")\n",
    "\n",
    "_NodeTestCases = []\n",
    "_TargetOpType = None\n",
    "_DiffOpTypes = None\n",
    "\n",
    "\n",
    "def _extract_value_info(\n",
    "    input: Union[List[Any], np.ndarray, None],\n",
    "    name: str,\n",
    "    type_proto: Optional[TypeProto] = None,\n",
    ") -> onnx.ValueInfoProto:\n",
    "    if type_proto is None:\n",
    "        if input is None:\n",
    "            raise NotImplementedError(\n",
    "                \"_extract_value_info: both input and type_proto arguments cannot be None.\"\n",
    "            )\n",
    "        elif isinstance(input, list):\n",
    "            elem_type = onnx.helper.np_dtype_to_tensor_dtype(input[0].dtype)\n",
    "            shape = None\n",
    "            tensor_type_proto = onnx.helper.make_tensor_type_proto(elem_type, shape)\n",
    "            type_proto = onnx.helper.make_sequence_type_proto(tensor_type_proto)\n",
    "        elif isinstance(input, TensorProto):\n",
    "            elem_type = input.data_type\n",
    "            shape = tuple(input.dims)\n",
    "            type_proto = onnx.helper.make_tensor_type_proto(elem_type, shape)\n",
    "        else:\n",
    "            elem_type = onnx.helper.np_dtype_to_tensor_dtype(input.dtype)\n",
    "            shape = input.shape\n",
    "            type_proto = onnx.helper.make_tensor_type_proto(elem_type, shape)\n",
    "\n",
    "    return onnx.helper.make_value_info(name, type_proto)\n",
    "\n",
    "def expect(\n",
    "    node: onnx.NodeProto,\n",
    "    inputs: Sequence[np.ndarray],\n",
    "    outputs: Sequence[np.ndarray],\n",
    "    name: str,\n",
    "    **kwargs: Any,\n",
    ") -> None:\n",
    "    # Builds the model\n",
    "    present_inputs = [x for x in node.input if (x != \"\")]\n",
    "    present_outputs = [x for x in node.output if (x != \"\")]\n",
    "    input_type_protos = [None] * len(inputs)\n",
    "    if \"input_type_protos\" in kwargs:\n",
    "        input_type_protos = kwargs[\"input_type_protos\"]\n",
    "        del kwargs[\"input_type_protos\"]\n",
    "    output_type_protos = [None] * len(outputs)\n",
    "    if \"output_type_protos\" in kwargs:\n",
    "        output_type_protos = kwargs[\"output_type_protos\"]\n",
    "        del kwargs[\"output_type_protos\"]\n",
    "    inputs_vi = [\n",
    "        _extract_value_info(arr, arr_name, input_type)\n",
    "        for arr, arr_name, input_type in zip(inputs, present_inputs, input_type_protos)\n",
    "    ]\n",
    "    outputs_vi = [\n",
    "        _extract_value_info(arr, arr_name, output_type)\n",
    "        for arr, arr_name, output_type in zip(\n",
    "            outputs, present_outputs, output_type_protos\n",
    "        )\n",
    "    ]\n",
    "    graph = onnx.helper.make_graph(\n",
    "        nodes=[node], name=name, inputs=inputs_vi, outputs=outputs_vi\n",
    "    )\n",
    "    kwargs[\"producer_name\"] = \"backend-test\"\n",
    "\n",
    "    if \"opset_imports\" not in kwargs:\n",
    "        # To make sure the model will be produced with the same opset_version after opset changes\n",
    "        # By default, it uses since_version as opset_version for produced models\n",
    "        produce_opset_version = onnx.defs.get_schema(\n",
    "            node.op_type, domain=node.domain\n",
    "        ).since_version\n",
    "        kwargs[\"opset_imports\"] = [\n",
    "            onnx.helper.make_operatorsetid(node.domain, produce_opset_version)\n",
    "        ]\n",
    "\n",
    "    model = onnx.helper.make_model_gen_version(graph, **kwargs)\n",
    "\n",
    "    # Checking the produces are the expected ones.\n",
    "    sess = onnxruntime.InferenceSession(model.SerializeToString(),\n",
    "                                        providers=[\"CPUExecutionProvider\"])\n",
    "    feeds = {name: value for name, value in zip(node.input, inputs)}\n",
    "    results = sess.run(None, feeds)\n",
    "    for expected, output in zip(outputs, results):\n",
    "        return (results, outputs)\n",
    "        return np.testing.assert_allclose(expected, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb0d5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "node = onnx.helper.make_node(\n",
    "    \"Squeeze\",\n",
    "    inputs=[\"x\", \"axes\"],\n",
    "    outputs=[\"y\"],\n",
    ")\n",
    "x = np.random.randn(1, 3, 4, 5).astype(np.float32)\n",
    "axes = np.array([0], dtype=np.int64)\n",
    "y = np.squeeze(x, axis=0)\n",
    "\n",
    "s = expect(node, inputs=[x, axes], outputs=[y], name=\"test_squeeze\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38af9809",
   "metadata": {},
   "outputs": [],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86328379",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "\n",
    "# Step 1: Define the ONNX node and create an ONNX graph\n",
    "node = onnx.helper.make_node(\n",
    "    \"Squeeze\",\n",
    "    inputs=[\"x\", \"axes\"],\n",
    "    outputs=[\"y\"],\n",
    ")\n",
    "graph = onnx.helper.make_graph([node], \"squeeze_graph\", inputs=[onnx.helper.make_tensor_value_info(\"x\", onnx.TensorProto.FLOAT, (1, 3, 4, 5)), onnx.helper.make_tensor_value_info(\"axes\", onnx.TensorProto.INT64, (1,))], outputs=[onnx.helper.make_tensor_value_info(\"y\", onnx.TensorProto.FLOAT, None)])\n",
    "\n",
    "# Step 2: Create an ONNX model with the graph\n",
    "onnx_model = onnx.helper.make_model(graph)\n",
    "\n",
    "# Save the ONNX model to a file\n",
    "onnx.save(onnx_model, \"squeeze_model.onnx\")\n",
    "\n",
    "# Step 3: Load the ONNX model with ONNX Runtime\n",
    "sess = ort.InferenceSession(\"squeeze_model.onnx\")\n",
    "\n",
    "# Step 4: Prepare input data\n",
    "x = np.random.randn(1, 3, 4, 5).astype(np.float32)\n",
    "axes = np.array([0], dtype=np.int64)\n",
    "\n",
    "# Step 5: Run inference\n",
    "output = sess.run([\"y\"], {\"x\": x, \"axes\": axes})\n",
    "\n",
    "# Print the output\n",
    "print(\"Output:\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c9b476",
   "metadata": {},
   "outputs": [],
   "source": [
    "output[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba5dd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90901d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define the ONNX node and create an ONNX graph\n",
    "node = onnx.helper.make_node(\n",
    "    \"Squeeze\",\n",
    "    inputs=[\"x\"],\n",
    "    outputs=[\"y\"],\n",
    "    axes=[0, 1]  # Squeeze along axes 0 and 1\n",
    ")\n",
    "graph = onnx.helper.make_graph([node], \"squeeze_graph\", inputs=[onnx.helper.make_tensor_value_info(\"x\", onnx.TensorProto.FLOAT, (1, 3, 4, 5))], outputs=[onnx.helper.make_tensor_value_info(\"y\", onnx.TensorProto.FLOAT, None)])\n",
    "\n",
    "# Step 2: Create an ONNX model with the graph\n",
    "onnx_model = onnx.helper.make_model(graph)\n",
    "\n",
    "# Save the ONNX model to a file\n",
    "onnx.save(onnx_model, \"squeeze_model.onnx\")\n",
    "\n",
    "# Step 3: Load the ONNX model with ONNX Runtime\n",
    "sess = ort.InferenceSession(\"squeeze_model.onnx\")\n",
    "\n",
    "# Step 4: Prepare input data\n",
    "x = np.random.randn(1, 3, 4, 5).astype(np.float32)\n",
    "\n",
    "# Step 5: Run inference\n",
    "output = sess.run([\"y\"], {\"x\": x})\n",
    "\n",
    "# Print the output\n",
    "print(\"Output:\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66efd122",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
