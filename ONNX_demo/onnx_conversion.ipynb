{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74dd3622",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import cv2\n",
    "import math\n",
    "import numpy as MOÃ¨Y\n",
    "import moviepy \n",
    "import torch\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "from my_alphaction.config import cfg\n",
    "from my_alphaction.modeling.detector import build_detection_model\n",
    "from my_alphaction.utils.checkpoint import ActionCheckpointer\n",
    "from my_alphaction.utils.comm import get_world_size\n",
    "\n",
    "\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b7ce27",
   "metadata": {},
   "source": [
    "### 1. CONFIG\n",
    "#### 1.1 Main Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6766963",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'VMAEv2'\n",
    "\n",
    "\n",
    "person_threshold = 0.6 # confidence threshold on actor. 0.6 is the defualt\n",
    "sampling_rate = 3 # sampling rate: 4 is the defualt\n",
    "top_k = 5 # number of actions per person\n",
    "video_path = '../input_dir/markt2_fight.mp4'\n",
    "\n",
    "slice_height = 800\n",
    "slice_width = 1000\n",
    "overlap_ratio = 0.1\n",
    "\n",
    "starting_frame_index = 100\n",
    "length_input = 200\n",
    "\n",
    "exp_dict = {'model_name': model_name,\n",
    "            'model_params': {'person_threshold': person_threshold, \n",
    "                             'sampling_rate': sampling_rate},\n",
    "            'orig_post_processing':{'top_k': top_k},\n",
    "            'aggregation': {'method': {}, \n",
    "                            'params': {}},\n",
    "            'video_path': video_path,\n",
    "            'slicing_params': {'slice_height': slice_height, \n",
    "                               'slice_width': slice_width, \n",
    "                               'overlap_ratio':overlap_ratio},\n",
    "            'video_params': {'st_frame_index': starting_frame_index, \n",
    "                             'length_input':length_input\n",
    "                             }\n",
    "           }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13831d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_name == 'VMAEv2':\n",
    "    config_file = '../config_files/VMAEv2-ViTB-16x4.yaml'\n",
    "if model_name == 'VMAE':\n",
    "    config_file = '../config_files/VMAE-ViTB-16x4.yaml'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f887add7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.merge_from_file(config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "310ceda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change model weight path\n",
    "if model_name == 'VMAEv2':\n",
    "    cfg.merge_from_list([\"MODEL.WEIGHT\", \"../checkpoints/VMAEv2_ViTB_16x4.pth\"])\n",
    "if model_name == 'VMAE':\n",
    "    cfg.merge_from_list([\"MODEL.WEIGHT\", \"../checkpoints/VMAE_ViTB_16x4.pth\"])\n",
    "\n",
    "# change output dir\n",
    "cfg.merge_from_list([\"OUTPUT_DIR\", \"../output_dir/\"])\n",
    "\n",
    "# change person threshold\n",
    "cfg.merge_from_list([\"MODEL.STM.PERSON_THRESHOLD\", person_threshold])\n",
    "\n",
    "# change sampling rate\n",
    "cfg.merge_from_list([\"DATA.SAMPLING_RATE\", sampling_rate])\n",
    "\n",
    "# change path for data_dir\n",
    "cfg.merge_from_list([\"DATA.PATH_TO_DATA_DIR\", \"/work/ava\"])\n",
    "\n",
    "# folder name of annotations\n",
    "cfg.merge_from_list([\"AVA.ANNOTATION_DIR\", \"annotations/\"])\n",
    "\n",
    "# file name of  frame_lists\n",
    "cfg.merge_from_list([\"AVA.TRAIN_LISTS\", ['sample.csv']])\n",
    "cfg.merge_from_list([\"AVA.TEST_LISTS\", ['sample.csv']])\n",
    "\n",
    "# file name of predicted_bboxes\n",
    "cfg.merge_from_list([\"AVA.TRAIN_GT_BOX_LISTS\", ['ava_sample_predicted_boxes.csv']])\n",
    "cfg.merge_from_list([\"AVA.TEST_GT_BOX_LISTS\", ['ava_sample_predicted_boxes.csv']])\n",
    "\n",
    "# file name of exlusions\n",
    "cfg.merge_from_list([\"AVA.EXCLUSION_FILE\", 'ava_sample_train_excluded_timestamps_v2.2.csv'])\n",
    "\n",
    "# number of batches in test scenario\n",
    "cfg.merge_from_list([\"TEST.VIDEOS_PER_BATCH\", 1])\n",
    "\n",
    "# number of workers\n",
    "cfg.merge_from_list([\"DATALOADER.NUM_WORKERS\", 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25e23e86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.ViT.USE_CHECKPOINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c0319f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.merge_from_list([\"ViT.USE_CHECKPOINT\", False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81d908cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.ViT.USE_CHECKPOINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33b27cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cfg.DATALOADER.SIZE_DIVISIBILITY:  32\n",
      "cfg.DATA.SAMPLING_RATE:  3\n",
      "cfg.DATA.NUM_FRAMES:  16\n",
      "self_seq_len:  48\n",
      "cfg.MODEL.STM.ACTION_CLASSES:  80\n",
      "Augmentation params:  [0.45, 0.45, 0.45] [0.225, 0.225, 0.225] False\n",
      "scale and flip params [256] 1333 False\n"
     ]
    }
   ],
   "source": [
    "debug = True\n",
    "if debug:\n",
    "    # The shape of model input should be divisible into this. Otherwise, padding 0 to left and bottum. \n",
    "    print(\"cfg.DATALOADER.SIZE_DIVISIBILITY: \", cfg.DATALOADER.SIZE_DIVISIBILITY)\n",
    "    \n",
    "    # Sampling rate in constructing the clips.\n",
    "    self_sample_rate =  cfg.DATA.SAMPLING_RATE\n",
    "    print(\"cfg.DATA.SAMPLING_RATE: \", cfg.DATA.SAMPLING_RATE)\n",
    "    \n",
    "    # Length of clip\n",
    "    self_video_length = cfg.DATA.NUM_FRAMES\n",
    "    print(\"cfg.DATA.NUM_FRAMES: \", cfg.DATA.NUM_FRAMES)\n",
    "    \n",
    "    # Length of sequence frames from which a clip is constructed.\n",
    "    self_seq_len = self_video_length * self_sample_rate\n",
    "    print(\"self_seq_len: \", self_seq_len)\n",
    "    \n",
    "    self_num_classes = cfg.MODEL.STM.ACTION_CLASSES\n",
    "    print(\"cfg.MODEL.STM.ACTION_CLASSES: \", self_num_classes)\n",
    "    \n",
    "    # Augmentation params.\n",
    "    self_data_mean = cfg.DATA.MEAN\n",
    "    self_data_std = cfg.DATA.STD\n",
    "    self_use_bgr = cfg.AVA.BGR\n",
    "    print(\"Augmentation params: \", self_data_mean, self_data_std, self_use_bgr)\n",
    "    \n",
    "    self_jitter_min_scale = cfg.DATA.TEST_MIN_SCALES\n",
    "    self_jitter_max_scale = cfg.DATA.TEST_MAX_SCALE\n",
    "    self_test_force_flip = cfg.AVA.TEST_FORCE_FLIP\n",
    "\n",
    "    print(\"scale and flip params\", self_jitter_min_scale, self_jitter_max_scale, self_test_force_flip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5baed4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_detection_model(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "895c6824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpointer = ActionCheckpointer(cfg, model, save_dir=\"../output_dir/\")\n",
    "checkpointer.load(cfg.MODEL.WEIGHT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50383232",
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    backbone_module = model._modules['backbone']\n",
    "    backbone_module = backbone_module.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77de51af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.onnx\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "000b871b",
   "metadata": {},
   "outputs": [],
   "source": [
    "slow_video = torch.randn(1, 3, 16, 256+64, 320+64).to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4db9132c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x forward input shape: torch.Size([1, 3, 16, 320, 384])\n",
      "x after patch_embed: torch.Size([1, 768, 8, 20, 24])\n",
      "x after flatten: torch.Size([1, 3840, 768])\n",
      "pos_embed shape before if: torch.Size([1, 1568, 768])\n",
      "pos_embed shape after if: torch.Size([8, 196, 768])\n",
      "gird_size: [14, 14]\n",
      "ws_s 20 24\n",
      "pos_embed shape after all: torch.Size([1, 3840, 768])\n"
     ]
    }
   ],
   "source": [
    "model(slow_video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c281478b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b8eaf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0a64e78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3c199bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x forward input shape: torch.Size([1, 3, 16, 256, 320])\n",
      "x after patch_embed: torch.Size([1, 768, 8, 16, 20])\n",
      "x after flatten: torch.Size([1, 2560, 768])\n",
      "pos_embed shape before if: torch.Size([1, 1568, 768])\n",
      "pos_embed shape after if: torch.Size([8, 196, 768])\n",
      "gird_size: [14, 14]\n",
      "ws_s tensor(16) tensor(20)\n",
      "pos_embed shape after all: torch.Size([1, 2560, 768])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "number of output names provided (3) exceeded number of outputs (0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39meval()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Export the model to ONNX with static batch size\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43monnx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexport\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m                  \u001b[49m\u001b[43margs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mslow_video\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel_decoder_stage_v2.onnx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m                  \u001b[49m\u001b[43minput_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mslow_video\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m                  \u001b[49m\u001b[43moutput_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput3\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mopset_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m     17\u001b[0m \u001b[43m                  \u001b[49m\u001b[38;5;66;43;03m#dynamic_axes={\"slow_video\": {0: \"batch_size\"}, \"whwh\": {0: \"batch_size\"}, \"output\": {0: \"batch_size\"}}\u001b[39;49;00m\n\u001b[1;32m     18\u001b[0m \u001b[43m                 \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/onnx/utils.py:504\u001b[0m, in \u001b[0;36mexport\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, custom_opsets, export_modules_as_functions)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;129m@_beartype\u001b[39m\u001b[38;5;241m.\u001b[39mbeartype\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexport\u001b[39m(\n\u001b[1;32m    188\u001b[0m     model: Union[torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule, torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mScriptModule, torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mScriptFunction],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    204\u001b[0m     export_modules_as_functions: Union[\u001b[38;5;28mbool\u001b[39m, Collection[Type[torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    205\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Exports a model into ONNX format.\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \n\u001b[1;32m    208\u001b[0m \u001b[38;5;124;03m    If ``model`` is not a :class:`torch.jit.ScriptModule` nor a\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    501\u001b[0m \u001b[38;5;124;03m            All errors are subclasses of :class:`errors.OnnxExporterError`.\u001b[39;00m\n\u001b[1;32m    502\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 504\u001b[0m     \u001b[43m_export\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    506\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[43m        \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexport_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    511\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    512\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    513\u001b[0m \u001b[43m        \u001b[49m\u001b[43moperator_export_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moperator_export_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    514\u001b[0m \u001b[43m        \u001b[49m\u001b[43mopset_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mopset_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_constant_folding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_constant_folding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdynamic_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_initializers_as_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_initializers_as_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_opsets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_opsets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexport_modules_as_functions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexport_modules_as_functions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/onnx/utils.py:1529\u001b[0m, in \u001b[0;36m_export\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, fixed_batch_size, custom_opsets, add_node_names, onnx_shape_inference, export_modules_as_functions)\u001b[0m\n\u001b[1;32m   1526\u001b[0m     dynamic_axes \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m   1527\u001b[0m _validate_dynamic_axes(dynamic_axes, model, input_names, output_names)\n\u001b[0;32m-> 1529\u001b[0m graph, params_dict, torch_out \u001b[38;5;241m=\u001b[39m \u001b[43m_model_to_graph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1530\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1531\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1532\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1533\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1534\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1535\u001b[0m \u001b[43m    \u001b[49m\u001b[43moperator_export_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1536\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_do_constant_folding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1537\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfixed_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfixed_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1538\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1539\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdynamic_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1540\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1542\u001b[0m \u001b[38;5;66;03m# TODO: Don't allocate a in-memory string for the protobuf\u001b[39;00m\n\u001b[1;32m   1543\u001b[0m defer_weight_export \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1544\u001b[0m     export_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _exporter_states\u001b[38;5;241m.\u001b[39mExportTypes\u001b[38;5;241m.\u001b[39mPROTOBUF_FILE\n\u001b[1;32m   1545\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/onnx/utils.py:1161\u001b[0m, in \u001b[0;36m_model_to_graph\u001b[0;34m(model, args, verbose, input_names, output_names, operator_export_type, do_constant_folding, _disable_torch_constant_prop, fixed_batch_size, training, dynamic_axes)\u001b[0m\n\u001b[1;32m   1152\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(out, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_quantized\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m out \u001b[38;5;129;01min\u001b[39;00m output_tensors):\n\u001b[1;32m   1153\u001b[0m         _C\u001b[38;5;241m.\u001b[39m_jit_pass_onnx_assign_output_shape(\n\u001b[1;32m   1154\u001b[0m             graph,\n\u001b[1;32m   1155\u001b[0m             output_tensors,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1158\u001b[0m             is_script,\n\u001b[1;32m   1159\u001b[0m         )\n\u001b[0;32m-> 1161\u001b[0m \u001b[43m_set_input_and_output_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1162\u001b[0m params_dict \u001b[38;5;241m=\u001b[39m _get_named_param_dict(graph, params)\n\u001b[1;32m   1164\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m training \u001b[38;5;241m==\u001b[39m _C_onnx\u001b[38;5;241m.\u001b[39mTrainingMode\u001b[38;5;241m.\u001b[39mEVAL:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/onnx/utils.py:1706\u001b[0m, in \u001b[0;36m_set_input_and_output_names\u001b[0;34m(graph, input_names, output_names)\u001b[0m\n\u001b[1;32m   1703\u001b[0m             node\u001b[38;5;241m.\u001b[39msetDebugName(name)\n\u001b[1;32m   1705\u001b[0m set_names(\u001b[38;5;28mlist\u001b[39m(graph\u001b[38;5;241m.\u001b[39minputs()), input_names, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1706\u001b[0m \u001b[43mset_names\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/onnx/utils.py:1683\u001b[0m, in \u001b[0;36m_set_input_and_output_names.<locals>.set_names\u001b[0;34m(node_list, name_list, descriptor)\u001b[0m\n\u001b[1;32m   1681\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1682\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(name_list) \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlen\u001b[39m(node_list):\n\u001b[0;32m-> 1683\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1684\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumber of \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m names provided (\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m) exceeded number of \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124ms (\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1685\u001b[0m         \u001b[38;5;241m%\u001b[39m (descriptor, \u001b[38;5;28mlen\u001b[39m(name_list), descriptor, \u001b[38;5;28mlen\u001b[39m(node_list))\n\u001b[1;32m   1686\u001b[0m     )\n\u001b[1;32m   1688\u001b[0m \u001b[38;5;66;03m# Mark if the output node DebugName is set before.\u001b[39;00m\n\u001b[1;32m   1689\u001b[0m output_node_set \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: number of output names provided (3) exceeded number of outputs (0)"
     ]
    }
   ],
   "source": [
    "slow_video = torch.randn(1, 3, 16, 256, 320).to('cpu')\n",
    "values = [[320., 256., 320., 256.]]\n",
    "\n",
    "# Create the tensor\n",
    "whwh = torch.tensor(values, device='cuda')\n",
    "\n",
    "model = model.eval().to('cpu')\n",
    "\n",
    "# Export the model to ONNX with static batch size\n",
    "torch.onnx.export(model, \n",
    "                  args = slow_video, \n",
    "                  f=\"model_decoder_stage_v2.onnx\", \n",
    "                  input_names=[\"slow_video\"], \n",
    "                  output_names=[\"output1\", \"output2\", \"output3\"],\n",
    "                  opset_version=16,\n",
    "                  verbose=False\n",
    "                  #dynamic_axes={\"slow_video\": {0: \"batch_size\"}, \"whwh\": {0: \"batch_size\"}, \"output\": {0: \"batch_size\"}}\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff0feb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472061bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.__dict__['_modules'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8622852",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.__dict__['_modules']['stm_head'].__dict__['_modules'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29cbba1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.__dict__['_modules']['stm_head'].__dict__['_modules']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e1f5d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae99016",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144a4c7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36853289",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553d818b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0f2c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_features = model(slow_video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fdb551",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "from onnx import helper\n",
    "from onnx import numpy_helper\n",
    "\n",
    "# Load the ONNX model\n",
    "model = onnx.load(\"whole_model_dyn_batch.onnx\")\n",
    "\n",
    "# Print a human-readable representation of the model's graph\n",
    "print(onnx.helper.printable_graph(model.graph))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4cc2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally, visualize the graph using external tools such as Netron\n",
    "\n",
    "# Perform inference on the model to ensure it produces expected outputs\n",
    "# Example inference code using onnxruntime\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "\n",
    "ort_session = ort.InferenceSession(\"whole_model_dyn_batch.onnx\")\n",
    "slow_video = np.random.randn(1, 3, 16, 256, 320).astype(np.float32)\n",
    "\n",
    "\n",
    "# Run inference\n",
    "outputs = ort_session.run([\"output\"], {\"slow_video\": slow_video})\n",
    "\n",
    "# Print the outputs\n",
    "print(outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18dcd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0356dfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76942ed0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da67f15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1445bbe7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93653dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "traced_model = torch.jit.trace(model, [slow_video])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3f5944",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.jit.optimized_execution(True):\n",
    "    scripted_model = torch.jit.script(model, [slow_video]).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10020fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.onnx.export(model, slow_video, \"model_v2.onnx\", do_constant_folding=False, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84836c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.onnx.export(traced_model, slow_video, \"traced_model.onnx\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aab42ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = onnx.load(\"your_model.onnx\")\n",
    "onnx.checker.check_model(my_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83c22bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# iterate through inputs of the graph\n",
    "for input in my_model.graph.input:\n",
    "    print (input.name, end=\": \")\n",
    "    # get type of input tensor\n",
    "    tensor_type = input.type.tensor_type\n",
    "    # check if it has a shape:\n",
    "    if (tensor_type.HasField(\"shape\")):\n",
    "        # iterate through dimensions of the shape:\n",
    "        for d in tensor_type.shape.dim:\n",
    "            # the dimension may have a definite (integer) value or a symbolic identifier or neither:\n",
    "            if (d.HasField(\"dim_value\")):\n",
    "                print (d.dim_value, end=\", \")  # known dimension\n",
    "            elif (d.HasField(\"dim_param\")):\n",
    "                print (d.dim_param, end=\", \")  # unknown dimension with symbolic name\n",
    "            else:\n",
    "                print (\"?\", end=\", \")  # unknown dimension with no name\n",
    "    else:\n",
    "        print (\"unknown rank\", end=\"\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47181c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract backbone and lateral_convs modules\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "backbone_module = model._modules['backbone']\n",
    "lateral_convs_module = model._modules['lateral_convs']\n",
    "\n",
    "# Assuming you want to combine them into a single model\n",
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self, backbone, lateral_convs):\n",
    "        super(CombinedModel, self).__init__()\n",
    "        self.backbone = backbone\n",
    "        self.lateral_convs = lateral_convs\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through the backbone and lateral_convs\n",
    "        backbone_output = self.backbone(x)\n",
    "        lateral_convs_output = self.lateral_convs(backbone_output)\n",
    "        return lateral_convs_output\n",
    "\n",
    "# Create the combined model instance\n",
    "combined_model = CombinedModel(backbone_module, lateral_convs_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f5f4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_input = torch.randn(1, 3, 16, 256, 320)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4be1da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_model([example_input])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdc2bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b7c28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "print(\"ONNX version:\", onnx.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed651488",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8716c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Sequence\n",
    "import numpy as np\n",
    "import onnx\n",
    "import onnxruntime\n",
    "import subprocess\n",
    "import sys\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "from typing import Any, Callable, Dict, List, Optional, Sequence, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import onnx\n",
    "from onnx.backend.test.case.test_case import TestCase\n",
    "from onnx.backend.test.case.utils import import_recursive\n",
    "from onnx.onnx_pb import (\n",
    "    AttributeProto,\n",
    "    FunctionProto,\n",
    "    GraphProto,\n",
    "    ModelProto,\n",
    "    NodeProto,\n",
    "    TensorProto,\n",
    "    TypeProto,\n",
    ")\n",
    "\n",
    "_NodeTestCases = []\n",
    "_TargetOpType = None\n",
    "_DiffOpTypes = None\n",
    "\n",
    "\n",
    "def _extract_value_info(\n",
    "    input: Union[List[Any], np.ndarray, None],\n",
    "    name: str,\n",
    "    type_proto: Optional[TypeProto] = None,\n",
    ") -> onnx.ValueInfoProto:\n",
    "    if type_proto is None:\n",
    "        if input is None:\n",
    "            raise NotImplementedError(\n",
    "                \"_extract_value_info: both input and type_proto arguments cannot be None.\"\n",
    "            )\n",
    "        elif isinstance(input, list):\n",
    "            elem_type = onnx.helper.np_dtype_to_tensor_dtype(input[0].dtype)\n",
    "            shape = None\n",
    "            tensor_type_proto = onnx.helper.make_tensor_type_proto(elem_type, shape)\n",
    "            type_proto = onnx.helper.make_sequence_type_proto(tensor_type_proto)\n",
    "        elif isinstance(input, TensorProto):\n",
    "            elem_type = input.data_type\n",
    "            shape = tuple(input.dims)\n",
    "            type_proto = onnx.helper.make_tensor_type_proto(elem_type, shape)\n",
    "        else:\n",
    "            elem_type = onnx.helper.np_dtype_to_tensor_dtype(input.dtype)\n",
    "            shape = input.shape\n",
    "            type_proto = onnx.helper.make_tensor_type_proto(elem_type, shape)\n",
    "\n",
    "    return onnx.helper.make_value_info(name, type_proto)\n",
    "\n",
    "def expect(\n",
    "    node: onnx.NodeProto,\n",
    "    inputs: Sequence[np.ndarray],\n",
    "    outputs: Sequence[np.ndarray],\n",
    "    name: str,\n",
    "    **kwargs: Any,\n",
    ") -> None:\n",
    "    # Builds the model\n",
    "    present_inputs = [x for x in node.input if (x != \"\")]\n",
    "    present_outputs = [x for x in node.output if (x != \"\")]\n",
    "    input_type_protos = [None] * len(inputs)\n",
    "    if \"input_type_protos\" in kwargs:\n",
    "        input_type_protos = kwargs[\"input_type_protos\"]\n",
    "        del kwargs[\"input_type_protos\"]\n",
    "    output_type_protos = [None] * len(outputs)\n",
    "    if \"output_type_protos\" in kwargs:\n",
    "        output_type_protos = kwargs[\"output_type_protos\"]\n",
    "        del kwargs[\"output_type_protos\"]\n",
    "    inputs_vi = [\n",
    "        _extract_value_info(arr, arr_name, input_type)\n",
    "        for arr, arr_name, input_type in zip(inputs, present_inputs, input_type_protos)\n",
    "    ]\n",
    "    outputs_vi = [\n",
    "        _extract_value_info(arr, arr_name, output_type)\n",
    "        for arr, arr_name, output_type in zip(\n",
    "            outputs, present_outputs, output_type_protos\n",
    "        )\n",
    "    ]\n",
    "    graph = onnx.helper.make_graph(\n",
    "        nodes=[node], name=name, inputs=inputs_vi, outputs=outputs_vi\n",
    "    )\n",
    "    kwargs[\"producer_name\"] = \"backend-test\"\n",
    "\n",
    "    if \"opset_imports\" not in kwargs:\n",
    "        # To make sure the model will be produced with the same opset_version after opset changes\n",
    "        # By default, it uses since_version as opset_version for produced models\n",
    "        produce_opset_version = onnx.defs.get_schema(\n",
    "            node.op_type, domain=node.domain\n",
    "        ).since_version\n",
    "        kwargs[\"opset_imports\"] = [\n",
    "            onnx.helper.make_operatorsetid(node.domain, produce_opset_version)\n",
    "        ]\n",
    "\n",
    "    model = onnx.helper.make_model_gen_version(graph, **kwargs)\n",
    "\n",
    "    # Checking the produces are the expected ones.\n",
    "    sess = onnxruntime.InferenceSession(model.SerializeToString(),\n",
    "                                        providers=[\"CPUExecutionProvider\"])\n",
    "    feeds = {name: value for name, value in zip(node.input, inputs)}\n",
    "    results = sess.run(None, feeds)\n",
    "    for expected, output in zip(outputs, results):\n",
    "        return (results, outputs)\n",
    "        return np.testing.assert_allclose(expected, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb0d5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "node = onnx.helper.make_node(\n",
    "    \"Squeeze\",\n",
    "    inputs=[\"x\", \"axes\"],\n",
    "    outputs=[\"y\"],\n",
    ")\n",
    "x = np.random.randn(1, 3, 4, 5).astype(np.float32)\n",
    "axes = np.array([0], dtype=np.int64)\n",
    "y = np.squeeze(x, axis=0)\n",
    "\n",
    "s = expect(node, inputs=[x, axes], outputs=[y], name=\"test_squeeze\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38af9809",
   "metadata": {},
   "outputs": [],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86328379",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "\n",
    "# Step 1: Define the ONNX node and create an ONNX graph\n",
    "node = onnx.helper.make_node(\n",
    "    \"Squeeze\",\n",
    "    inputs=[\"x\", \"axes\"],\n",
    "    outputs=[\"y\"],\n",
    ")\n",
    "graph = onnx.helper.make_graph([node], \"squeeze_graph\", inputs=[onnx.helper.make_tensor_value_info(\"x\", onnx.TensorProto.FLOAT, (1, 3, 4, 5)), onnx.helper.make_tensor_value_info(\"axes\", onnx.TensorProto.INT64, (1,))], outputs=[onnx.helper.make_tensor_value_info(\"y\", onnx.TensorProto.FLOAT, None)])\n",
    "\n",
    "# Step 2: Create an ONNX model with the graph\n",
    "onnx_model = onnx.helper.make_model(graph)\n",
    "\n",
    "# Save the ONNX model to a file\n",
    "onnx.save(onnx_model, \"squeeze_model.onnx\")\n",
    "\n",
    "# Step 3: Load the ONNX model with ONNX Runtime\n",
    "sess = ort.InferenceSession(\"squeeze_model.onnx\")\n",
    "\n",
    "# Step 4: Prepare input data\n",
    "x = np.random.randn(1, 3, 4, 5).astype(np.float32)\n",
    "axes = np.array([0], dtype=np.int64)\n",
    "\n",
    "# Step 5: Run inference\n",
    "output = sess.run([\"y\"], {\"x\": x, \"axes\": axes})\n",
    "\n",
    "# Print the output\n",
    "print(\"Output:\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c9b476",
   "metadata": {},
   "outputs": [],
   "source": [
    "output[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba5dd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90901d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define the ONNX node and create an ONNX graph\n",
    "node = onnx.helper.make_node(\n",
    "    \"Squeeze\",\n",
    "    inputs=[\"x\"],\n",
    "    outputs=[\"y\"],\n",
    "    axes=[0, 1]  # Squeeze along axes 0 and 1\n",
    ")\n",
    "graph = onnx.helper.make_graph([node], \"squeeze_graph\", inputs=[onnx.helper.make_tensor_value_info(\"x\", onnx.TensorProto.FLOAT, (1, 3, 4, 5))], outputs=[onnx.helper.make_tensor_value_info(\"y\", onnx.TensorProto.FLOAT, None)])\n",
    "\n",
    "# Step 2: Create an ONNX model with the graph\n",
    "onnx_model = onnx.helper.make_model(graph)\n",
    "\n",
    "# Save the ONNX model to a file\n",
    "onnx.save(onnx_model, \"squeeze_model.onnx\")\n",
    "\n",
    "# Step 3: Load the ONNX model with ONNX Runtime\n",
    "sess = ort.InferenceSession(\"squeeze_model.onnx\")\n",
    "\n",
    "# Step 4: Prepare input data\n",
    "x = np.random.randn(1, 3, 4, 5).astype(np.float32)\n",
    "\n",
    "# Step 5: Run inference\n",
    "output = sess.run([\"y\"], {\"x\": x})\n",
    "\n",
    "# Print the output\n",
    "print(\"Output:\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66efd122",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def get_variable_info(*args):\n",
    "    variable_info = {}\n",
    "    \n",
    "    def get_info(arg):\n",
    "        info = {}\n",
    "        if isinstance(arg, torch.Tensor):\n",
    "            info[\"type\"] = \"Tensor\"\n",
    "            info[\"shape\"] = tuple(arg.shape)\n",
    "        elif isinstance(arg, (list, tuple)):\n",
    "            info[\"length\"] = len(arg)\n",
    "            if arg:  # Check if the list or tuple is not empty\n",
    "                item_shapes = []\n",
    "                for item in arg:\n",
    "                    item_shapes.append(get_info(item))\n",
    "                info[\"item_shapes\"] = item_shapes\n",
    "        else:\n",
    "            info[\"type\"] = type(arg).__name__\n",
    "        return info\n",
    "    \n",
    "    for i, arg_value in enumerate(args):\n",
    "        arg_name = f\"arg_{i+1}\"  # Create a name for the argument\n",
    "        variable_info[arg_name] = get_info(arg_value)\n",
    "    \n",
    "    return variable_info\n",
    "\n",
    "# Example usage\n",
    "x = torch.randn(1, 3, 16, 256, 320)\n",
    "y = torch.randn(2, 3, 4)\n",
    "z = [torch.randn(2, 3), torch.randn(3, 4, 5)]\n",
    "\n",
    "info = get_variable_info(x, y, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd64af9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671fd858",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class Foo(torch.nn.Module):\n",
    "    def forward(self, tensor):\n",
    "        # It is data dependent\n",
    "        # Trace will only work with one path\n",
    "        if tensor.max() > 0.5:\n",
    "            return tensor ** 2\n",
    "        return tensor\n",
    "\n",
    "\n",
    "model = Foo()\n",
    "traced = torch.jit.script(model) # No warnings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6956d99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "traced_v2= torch.jit.trace(model, torch.randn(10)) # Warning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747bb327",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = torch.randn(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5448a9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = traced(test_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e02fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f593b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "traced_v2(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a01f304",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = torch.nn.Linear(20, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1333a154",
   "metadata": {},
   "outputs": [],
   "source": [
    "m._parameters['weight'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5ee4b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
