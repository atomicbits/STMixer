{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74dd3622",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import cv2\n",
    "import math\n",
    "import numpy as MOÃ¨Y\n",
    "import moviepy \n",
    "import torch\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "from my_alphaction.config import cfg\n",
    "from my_alphaction.modeling.detector import build_detection_model\n",
    "from my_alphaction.utils.checkpoint import ActionCheckpointer\n",
    "from my_alphaction.utils.comm import get_world_size\n",
    "\n",
    "\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b7ce27",
   "metadata": {},
   "source": [
    "### 1. CONFIG\n",
    "#### 1.1 Main Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6766963",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'VMAEv2'\n",
    "\n",
    "\n",
    "person_threshold = 0.6 # confidence threshold on actor. 0.6 is the defualt\n",
    "sampling_rate = 3 # sampling rate: 4 is the defualt\n",
    "top_k = 5 # number of actions per person\n",
    "video_path = '../input_dir/markt2_fight.mp4'\n",
    "\n",
    "slice_height = 800\n",
    "slice_width = 1000\n",
    "overlap_ratio = 0.1\n",
    "\n",
    "starting_frame_index = 100\n",
    "length_input = 200\n",
    "\n",
    "exp_dict = {'model_name': model_name,\n",
    "            'model_params': {'person_threshold': person_threshold, \n",
    "                             'sampling_rate': sampling_rate},\n",
    "            'orig_post_processing':{'top_k': top_k},\n",
    "            'aggregation': {'method': {}, \n",
    "                            'params': {}},\n",
    "            'video_path': video_path,\n",
    "            'slicing_params': {'slice_height': slice_height, \n",
    "                               'slice_width': slice_width, \n",
    "                               'overlap_ratio':overlap_ratio},\n",
    "            'video_params': {'st_frame_index': starting_frame_index, \n",
    "                             'length_input':length_input\n",
    "                             }\n",
    "           }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13831d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_name == 'VMAEv2':\n",
    "    config_file = '../config_files/VMAEv2-ViTB-16x4.yaml'\n",
    "if model_name == 'VMAE':\n",
    "    config_file = '../config_files/VMAE-ViTB-16x4.yaml'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f887add7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.merge_from_file(config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "310ceda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change model weight path\n",
    "if model_name == 'VMAEv2':\n",
    "    cfg.merge_from_list([\"MODEL.WEIGHT\", \"../checkpoints/VMAEv2_ViTB_16x4.pth\"])\n",
    "if model_name == 'VMAE':\n",
    "    cfg.merge_from_list([\"MODEL.WEIGHT\", \"../checkpoints/VMAE_ViTB_16x4.pth\"])\n",
    "\n",
    "# change output dir\n",
    "cfg.merge_from_list([\"OUTPUT_DIR\", \"../output_dir/\"])\n",
    "\n",
    "# change person threshold\n",
    "cfg.merge_from_list([\"MODEL.STM.PERSON_THRESHOLD\", person_threshold])\n",
    "\n",
    "# change sampling rate\n",
    "cfg.merge_from_list([\"DATA.SAMPLING_RATE\", sampling_rate])\n",
    "\n",
    "# change path for data_dir\n",
    "cfg.merge_from_list([\"DATA.PATH_TO_DATA_DIR\", \"/work/ava\"])\n",
    "\n",
    "# folder name of annotations\n",
    "cfg.merge_from_list([\"AVA.ANNOTATION_DIR\", \"annotations/\"])\n",
    "\n",
    "# file name of  frame_lists\n",
    "cfg.merge_from_list([\"AVA.TRAIN_LISTS\", ['sample.csv']])\n",
    "cfg.merge_from_list([\"AVA.TEST_LISTS\", ['sample.csv']])\n",
    "\n",
    "# file name of predicted_bboxes\n",
    "cfg.merge_from_list([\"AVA.TRAIN_GT_BOX_LISTS\", ['ava_sample_predicted_boxes.csv']])\n",
    "cfg.merge_from_list([\"AVA.TEST_GT_BOX_LISTS\", ['ava_sample_predicted_boxes.csv']])\n",
    "\n",
    "# file name of exlusions\n",
    "cfg.merge_from_list([\"AVA.EXCLUSION_FILE\", 'ava_sample_train_excluded_timestamps_v2.2.csv'])\n",
    "\n",
    "# number of batches in test scenario\n",
    "cfg.merge_from_list([\"TEST.VIDEOS_PER_BATCH\", 1])\n",
    "\n",
    "# number of workers\n",
    "cfg.merge_from_list([\"DATALOADER.NUM_WORKERS\", 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25e23e86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.ViT.USE_CHECKPOINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c0319f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.merge_from_list([\"ViT.USE_CHECKPOINT\", False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81d908cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.ViT.USE_CHECKPOINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33b27cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cfg.DATALOADER.SIZE_DIVISIBILITY:  32\n",
      "cfg.DATA.SAMPLING_RATE:  3\n",
      "cfg.DATA.NUM_FRAMES:  16\n",
      "self_seq_len:  48\n",
      "cfg.MODEL.STM.ACTION_CLASSES:  80\n",
      "Augmentation params:  [0.45, 0.45, 0.45] [0.225, 0.225, 0.225] False\n",
      "scale and flip params [256] 1333 False\n"
     ]
    }
   ],
   "source": [
    "debug = True\n",
    "if debug:\n",
    "    # The shape of model input should be divisible into this. Otherwise, padding 0 to left and bottum. \n",
    "    print(\"cfg.DATALOADER.SIZE_DIVISIBILITY: \", cfg.DATALOADER.SIZE_DIVISIBILITY)\n",
    "    \n",
    "    # Sampling rate in constructing the clips.\n",
    "    self_sample_rate =  cfg.DATA.SAMPLING_RATE\n",
    "    print(\"cfg.DATA.SAMPLING_RATE: \", cfg.DATA.SAMPLING_RATE)\n",
    "    \n",
    "    # Length of clip\n",
    "    self_video_length = cfg.DATA.NUM_FRAMES\n",
    "    print(\"cfg.DATA.NUM_FRAMES: \", cfg.DATA.NUM_FRAMES)\n",
    "    \n",
    "    # Length of sequence frames from which a clip is constructed.\n",
    "    self_seq_len = self_video_length * self_sample_rate\n",
    "    print(\"self_seq_len: \", self_seq_len)\n",
    "    \n",
    "    self_num_classes = cfg.MODEL.STM.ACTION_CLASSES\n",
    "    print(\"cfg.MODEL.STM.ACTION_CLASSES: \", self_num_classes)\n",
    "    \n",
    "    # Augmentation params.\n",
    "    self_data_mean = cfg.DATA.MEAN\n",
    "    self_data_std = cfg.DATA.STD\n",
    "    self_use_bgr = cfg.AVA.BGR\n",
    "    print(\"Augmentation params: \", self_data_mean, self_data_std, self_use_bgr)\n",
    "    \n",
    "    self_jitter_min_scale = cfg.DATA.TEST_MIN_SCALES\n",
    "    self_jitter_max_scale = cfg.DATA.TEST_MAX_SCALE\n",
    "    self_test_force_flip = cfg.AVA.TEST_FORCE_FLIP\n",
    "\n",
    "    print(\"scale and flip params\", self_jitter_min_scale, self_jitter_max_scale, self_test_force_flip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5baed4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_detection_model(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "895c6824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpointer = ActionCheckpointer(cfg, model, save_dir=\"../output_dir/\")\n",
    "checkpointer.load(cfg.MODEL.WEIGHT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50383232",
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    backbone_module = model._modules['backbone']\n",
    "    backbone_module = backbone_module.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77de51af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.onnx\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "000b871b",
   "metadata": {},
   "outputs": [],
   "source": [
    "slow_video = torch.randn(1, 3, 16, 256+64, 320+64).to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4db9132c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x forward input shape: torch.Size([1, 3, 16, 320, 384])\n",
      "x after patch_embed: torch.Size([1, 768, 8, 20, 24])\n",
      "x after flatten: torch.Size([1, 3840, 768])\n",
      "pos_embed shape before if: torch.Size([1, 1568, 768])\n",
      "pos_embed shape after if: torch.Size([8, 196, 768])\n",
      "gird_size: [14, 14]\n",
      "ws_s 20 24\n",
      "pos_embed shape after all: torch.Size([1, 3840, 768])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[tensor([[[ -3.7434, -11.8778,  -4.3906,  ...,  -8.0285,  -0.9966,  -0.3563],\n",
       "          [ -3.8643, -11.9442,  -4.3439,  ...,  -8.2044,  -1.0482,  -0.5466],\n",
       "          [ -3.7065, -11.8679,  -4.3757,  ...,  -8.0272,  -1.0268,  -0.5813],\n",
       "          ...,\n",
       "          [ -3.7255, -11.8799,  -4.4662,  ...,  -8.0591,  -1.0265,  -0.5829],\n",
       "          [ -3.7719, -12.0909,  -4.1032,  ...,  -8.0336,  -1.0050,  -0.4918],\n",
       "          [ -3.7142, -12.0353,  -4.1871,  ...,  -8.0224,  -0.9460,  -0.5059]]],\n",
       "        grad_fn=<ViewBackward0>),\n",
       " tensor([[[ -6.1144, -13.9819,  -6.8985,  ...,  -8.7508,  -2.1330,  -2.0613],\n",
       "          [ -5.6654, -13.6685,  -6.7246,  ...,  -8.8098,  -2.3053,  -2.6272],\n",
       "          [ -5.8000, -13.7061,  -6.7167,  ...,  -8.6228,  -2.2140,  -2.4665],\n",
       "          ...,\n",
       "          [ -5.9854, -13.8458,  -6.7762,  ...,  -8.7279,  -2.1971,  -2.3371],\n",
       "          [ -5.5259, -13.3174,  -6.8426,  ...,  -8.7278,  -2.3362,  -2.4527],\n",
       "          [ -5.8312, -13.7180,  -6.8018,  ...,  -8.6408,  -2.2620,  -2.4025]]],\n",
       "        grad_fn=<ViewBackward0>),\n",
       " tensor([[[ -4.7037, -11.5107,  -6.2714,  ...,  -8.7903,  -1.5043,  -0.5468],\n",
       "          [ -5.0278, -11.0216,  -6.0634,  ...,  -9.0626,  -1.4971,  -0.7747],\n",
       "          [ -5.2505, -11.1283,  -5.9318,  ...,  -9.1048,  -1.7880,  -0.7138],\n",
       "          ...,\n",
       "          [ -5.2593, -11.6493,  -6.2373,  ...,  -9.3524,  -2.0667,  -0.6769],\n",
       "          [ -5.1034, -10.8828,  -5.9040,  ...,  -9.0993,  -1.5056,  -0.7826],\n",
       "          [ -5.1530, -11.0187,  -6.0453,  ...,  -9.0500,  -1.6403,  -0.7211]]],\n",
       "        grad_fn=<ViewBackward0>),\n",
       " tensor([[[ -3.0344, -11.9087,  -7.2216,  ...,  -8.1848,   1.0940,  -1.3599],\n",
       "          [ -3.6521, -12.2993,  -7.6544,  ...,  -8.8071,   0.8622,  -0.9268],\n",
       "          [ -3.6190, -12.3385,  -7.3002,  ...,  -8.6243,   0.8196,  -0.9900],\n",
       "          ...,\n",
       "          [ -3.4003, -12.3090,  -7.0829,  ...,  -8.1818,   0.9450,  -1.3662],\n",
       "          [ -3.5599, -12.3200,  -7.4351,  ...,  -8.8272,   0.8984,  -0.9308],\n",
       "          [ -3.5024, -12.4362,  -7.4256,  ...,  -8.7427,   0.9890,  -0.9642]]],\n",
       "        grad_fn=<ViewBackward0>),\n",
       " tensor([[[-3.4085, -9.7146, -4.7813,  ..., -9.2441, -0.7920, -2.8198],\n",
       "          [-3.6479, -9.4317, -5.2302,  ..., -9.2405, -0.9077, -2.9587],\n",
       "          [-3.6388, -9.3306, -4.9694,  ..., -9.1895, -0.8657, -3.0413],\n",
       "          ...,\n",
       "          [-3.7435, -9.3434, -4.4365,  ..., -9.0280, -0.9938, -3.1077],\n",
       "          [-3.6076, -9.4698, -5.1505,  ..., -9.1525, -0.9563, -2.9050],\n",
       "          [-3.5861, -9.5844, -5.0721,  ..., -9.2140, -0.7438, -2.9809]]],\n",
       "        grad_fn=<ViewBackward0>),\n",
       " tensor([[[ -4.4742, -11.8870,  -6.3313,  ..., -10.0240,   0.0386,  -3.6667],\n",
       "          [ -4.7364, -11.3941,  -6.4393,  ..., -10.0160,  -0.3318,  -4.1596],\n",
       "          [ -4.6124, -11.3717,  -6.3471,  ..., -10.0161,  -0.1919,  -3.9766],\n",
       "          ...,\n",
       "          [ -4.3515, -11.2430,  -6.2347,  ...,  -9.9427,  -0.0377,  -3.8368],\n",
       "          [ -4.8403, -11.4647,  -6.4428,  ..., -10.0231,  -0.3354,  -4.1488],\n",
       "          [ -4.7086, -11.5637,  -6.4044,  ..., -10.0885,  -0.1611,  -4.0049]]],\n",
       "        grad_fn=<ViewBackward0>)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(slow_video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c281478b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b8eaf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0a64e78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c199bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "slow_video = torch.randn(1, 3, 16, 256, 320).to('cpu')\n",
    "values = [[320., 256., 320., 256.]]\n",
    "\n",
    "# Create the tensor\n",
    "whwh = torch.tensor(values, device='cuda')\n",
    "\n",
    "model = model.eval().to('cpu')\n",
    "\n",
    "# Export the model to ONNX with static batch size\n",
    "torch.onnx.export(model, \n",
    "                  args = slow_video, \n",
    "                  f=\"model_decoder_stage_v2.onnx\", \n",
    "                  input_names=[\"slow_video\"], \n",
    "                  output_names=[\"output\"],\n",
    "                  opset_version=16,\n",
    "                  verbose=False\n",
    "                  #dynamic_axes={\"slow_video\": {0: \"batch_size\"}, \"whwh\": {0: \"batch_size\"}, \"output\": {0: \"batch_size\"}}\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff0feb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472061bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.__dict__['_modules'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8622852",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.__dict__['_modules']['stm_head'].__dict__['_modules'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29cbba1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.__dict__['_modules']['stm_head'].__dict__['_modules']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e1f5d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae99016",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144a4c7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36853289",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553d818b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0f2c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_features = model(slow_video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fdb551",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "from onnx import helper\n",
    "from onnx import numpy_helper\n",
    "\n",
    "# Load the ONNX model\n",
    "model = onnx.load(\"whole_model_dyn_batch.onnx\")\n",
    "\n",
    "# Print a human-readable representation of the model's graph\n",
    "print(onnx.helper.printable_graph(model.graph))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4cc2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally, visualize the graph using external tools such as Netron\n",
    "\n",
    "# Perform inference on the model to ensure it produces expected outputs\n",
    "# Example inference code using onnxruntime\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "\n",
    "ort_session = ort.InferenceSession(\"whole_model_dyn_batch.onnx\")\n",
    "slow_video = np.random.randn(1, 3, 16, 256, 320).astype(np.float32)\n",
    "\n",
    "\n",
    "# Run inference\n",
    "outputs = ort_session.run([\"output\"], {\"slow_video\": slow_video})\n",
    "\n",
    "# Print the outputs\n",
    "print(outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18dcd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0356dfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76942ed0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da67f15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1445bbe7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93653dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "traced_model = torch.jit.trace(model, [slow_video])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3f5944",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.jit.optimized_execution(True):\n",
    "    scripted_model = torch.jit.script(model, [slow_video]).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10020fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.onnx.export(model, slow_video, \"model_v2.onnx\", do_constant_folding=False, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84836c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.onnx.export(traced_model, slow_video, \"traced_model.onnx\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aab42ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = onnx.load(\"your_model.onnx\")\n",
    "onnx.checker.check_model(my_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83c22bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# iterate through inputs of the graph\n",
    "for input in my_model.graph.input:\n",
    "    print (input.name, end=\": \")\n",
    "    # get type of input tensor\n",
    "    tensor_type = input.type.tensor_type\n",
    "    # check if it has a shape:\n",
    "    if (tensor_type.HasField(\"shape\")):\n",
    "        # iterate through dimensions of the shape:\n",
    "        for d in tensor_type.shape.dim:\n",
    "            # the dimension may have a definite (integer) value or a symbolic identifier or neither:\n",
    "            if (d.HasField(\"dim_value\")):\n",
    "                print (d.dim_value, end=\", \")  # known dimension\n",
    "            elif (d.HasField(\"dim_param\")):\n",
    "                print (d.dim_param, end=\", \")  # unknown dimension with symbolic name\n",
    "            else:\n",
    "                print (\"?\", end=\", \")  # unknown dimension with no name\n",
    "    else:\n",
    "        print (\"unknown rank\", end=\"\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47181c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract backbone and lateral_convs modules\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "backbone_module = model._modules['backbone']\n",
    "lateral_convs_module = model._modules['lateral_convs']\n",
    "\n",
    "# Assuming you want to combine them into a single model\n",
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self, backbone, lateral_convs):\n",
    "        super(CombinedModel, self).__init__()\n",
    "        self.backbone = backbone\n",
    "        self.lateral_convs = lateral_convs\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through the backbone and lateral_convs\n",
    "        backbone_output = self.backbone(x)\n",
    "        lateral_convs_output = self.lateral_convs(backbone_output)\n",
    "        return lateral_convs_output\n",
    "\n",
    "# Create the combined model instance\n",
    "combined_model = CombinedModel(backbone_module, lateral_convs_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f5f4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_input = torch.randn(1, 3, 16, 256, 320)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4be1da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_model([example_input])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdc2bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b7c28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "print(\"ONNX version:\", onnx.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed651488",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8716c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Sequence\n",
    "import numpy as np\n",
    "import onnx\n",
    "import onnxruntime\n",
    "import subprocess\n",
    "import sys\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "from typing import Any, Callable, Dict, List, Optional, Sequence, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import onnx\n",
    "from onnx.backend.test.case.test_case import TestCase\n",
    "from onnx.backend.test.case.utils import import_recursive\n",
    "from onnx.onnx_pb import (\n",
    "    AttributeProto,\n",
    "    FunctionProto,\n",
    "    GraphProto,\n",
    "    ModelProto,\n",
    "    NodeProto,\n",
    "    TensorProto,\n",
    "    TypeProto,\n",
    ")\n",
    "\n",
    "_NodeTestCases = []\n",
    "_TargetOpType = None\n",
    "_DiffOpTypes = None\n",
    "\n",
    "\n",
    "def _extract_value_info(\n",
    "    input: Union[List[Any], np.ndarray, None],\n",
    "    name: str,\n",
    "    type_proto: Optional[TypeProto] = None,\n",
    ") -> onnx.ValueInfoProto:\n",
    "    if type_proto is None:\n",
    "        if input is None:\n",
    "            raise NotImplementedError(\n",
    "                \"_extract_value_info: both input and type_proto arguments cannot be None.\"\n",
    "            )\n",
    "        elif isinstance(input, list):\n",
    "            elem_type = onnx.helper.np_dtype_to_tensor_dtype(input[0].dtype)\n",
    "            shape = None\n",
    "            tensor_type_proto = onnx.helper.make_tensor_type_proto(elem_type, shape)\n",
    "            type_proto = onnx.helper.make_sequence_type_proto(tensor_type_proto)\n",
    "        elif isinstance(input, TensorProto):\n",
    "            elem_type = input.data_type\n",
    "            shape = tuple(input.dims)\n",
    "            type_proto = onnx.helper.make_tensor_type_proto(elem_type, shape)\n",
    "        else:\n",
    "            elem_type = onnx.helper.np_dtype_to_tensor_dtype(input.dtype)\n",
    "            shape = input.shape\n",
    "            type_proto = onnx.helper.make_tensor_type_proto(elem_type, shape)\n",
    "\n",
    "    return onnx.helper.make_value_info(name, type_proto)\n",
    "\n",
    "def expect(\n",
    "    node: onnx.NodeProto,\n",
    "    inputs: Sequence[np.ndarray],\n",
    "    outputs: Sequence[np.ndarray],\n",
    "    name: str,\n",
    "    **kwargs: Any,\n",
    ") -> None:\n",
    "    # Builds the model\n",
    "    present_inputs = [x for x in node.input if (x != \"\")]\n",
    "    present_outputs = [x for x in node.output if (x != \"\")]\n",
    "    input_type_protos = [None] * len(inputs)\n",
    "    if \"input_type_protos\" in kwargs:\n",
    "        input_type_protos = kwargs[\"input_type_protos\"]\n",
    "        del kwargs[\"input_type_protos\"]\n",
    "    output_type_protos = [None] * len(outputs)\n",
    "    if \"output_type_protos\" in kwargs:\n",
    "        output_type_protos = kwargs[\"output_type_protos\"]\n",
    "        del kwargs[\"output_type_protos\"]\n",
    "    inputs_vi = [\n",
    "        _extract_value_info(arr, arr_name, input_type)\n",
    "        for arr, arr_name, input_type in zip(inputs, present_inputs, input_type_protos)\n",
    "    ]\n",
    "    outputs_vi = [\n",
    "        _extract_value_info(arr, arr_name, output_type)\n",
    "        for arr, arr_name, output_type in zip(\n",
    "            outputs, present_outputs, output_type_protos\n",
    "        )\n",
    "    ]\n",
    "    graph = onnx.helper.make_graph(\n",
    "        nodes=[node], name=name, inputs=inputs_vi, outputs=outputs_vi\n",
    "    )\n",
    "    kwargs[\"producer_name\"] = \"backend-test\"\n",
    "\n",
    "    if \"opset_imports\" not in kwargs:\n",
    "        # To make sure the model will be produced with the same opset_version after opset changes\n",
    "        # By default, it uses since_version as opset_version for produced models\n",
    "        produce_opset_version = onnx.defs.get_schema(\n",
    "            node.op_type, domain=node.domain\n",
    "        ).since_version\n",
    "        kwargs[\"opset_imports\"] = [\n",
    "            onnx.helper.make_operatorsetid(node.domain, produce_opset_version)\n",
    "        ]\n",
    "\n",
    "    model = onnx.helper.make_model_gen_version(graph, **kwargs)\n",
    "\n",
    "    # Checking the produces are the expected ones.\n",
    "    sess = onnxruntime.InferenceSession(model.SerializeToString(),\n",
    "                                        providers=[\"CPUExecutionProvider\"])\n",
    "    feeds = {name: value for name, value in zip(node.input, inputs)}\n",
    "    results = sess.run(None, feeds)\n",
    "    for expected, output in zip(outputs, results):\n",
    "        return (results, outputs)\n",
    "        return np.testing.assert_allclose(expected, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb0d5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "node = onnx.helper.make_node(\n",
    "    \"Squeeze\",\n",
    "    inputs=[\"x\", \"axes\"],\n",
    "    outputs=[\"y\"],\n",
    ")\n",
    "x = np.random.randn(1, 3, 4, 5).astype(np.float32)\n",
    "axes = np.array([0], dtype=np.int64)\n",
    "y = np.squeeze(x, axis=0)\n",
    "\n",
    "s = expect(node, inputs=[x, axes], outputs=[y], name=\"test_squeeze\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38af9809",
   "metadata": {},
   "outputs": [],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86328379",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "\n",
    "# Step 1: Define the ONNX node and create an ONNX graph\n",
    "node = onnx.helper.make_node(\n",
    "    \"Squeeze\",\n",
    "    inputs=[\"x\", \"axes\"],\n",
    "    outputs=[\"y\"],\n",
    ")\n",
    "graph = onnx.helper.make_graph([node], \"squeeze_graph\", inputs=[onnx.helper.make_tensor_value_info(\"x\", onnx.TensorProto.FLOAT, (1, 3, 4, 5)), onnx.helper.make_tensor_value_info(\"axes\", onnx.TensorProto.INT64, (1,))], outputs=[onnx.helper.make_tensor_value_info(\"y\", onnx.TensorProto.FLOAT, None)])\n",
    "\n",
    "# Step 2: Create an ONNX model with the graph\n",
    "onnx_model = onnx.helper.make_model(graph)\n",
    "\n",
    "# Save the ONNX model to a file\n",
    "onnx.save(onnx_model, \"squeeze_model.onnx\")\n",
    "\n",
    "# Step 3: Load the ONNX model with ONNX Runtime\n",
    "sess = ort.InferenceSession(\"squeeze_model.onnx\")\n",
    "\n",
    "# Step 4: Prepare input data\n",
    "x = np.random.randn(1, 3, 4, 5).astype(np.float32)\n",
    "axes = np.array([0], dtype=np.int64)\n",
    "\n",
    "# Step 5: Run inference\n",
    "output = sess.run([\"y\"], {\"x\": x, \"axes\": axes})\n",
    "\n",
    "# Print the output\n",
    "print(\"Output:\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c9b476",
   "metadata": {},
   "outputs": [],
   "source": [
    "output[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba5dd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90901d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define the ONNX node and create an ONNX graph\n",
    "node = onnx.helper.make_node(\n",
    "    \"Squeeze\",\n",
    "    inputs=[\"x\"],\n",
    "    outputs=[\"y\"],\n",
    "    axes=[0, 1]  # Squeeze along axes 0 and 1\n",
    ")\n",
    "graph = onnx.helper.make_graph([node], \"squeeze_graph\", inputs=[onnx.helper.make_tensor_value_info(\"x\", onnx.TensorProto.FLOAT, (1, 3, 4, 5))], outputs=[onnx.helper.make_tensor_value_info(\"y\", onnx.TensorProto.FLOAT, None)])\n",
    "\n",
    "# Step 2: Create an ONNX model with the graph\n",
    "onnx_model = onnx.helper.make_model(graph)\n",
    "\n",
    "# Save the ONNX model to a file\n",
    "onnx.save(onnx_model, \"squeeze_model.onnx\")\n",
    "\n",
    "# Step 3: Load the ONNX model with ONNX Runtime\n",
    "sess = ort.InferenceSession(\"squeeze_model.onnx\")\n",
    "\n",
    "# Step 4: Prepare input data\n",
    "x = np.random.randn(1, 3, 4, 5).astype(np.float32)\n",
    "\n",
    "# Step 5: Run inference\n",
    "output = sess.run([\"y\"], {\"x\": x})\n",
    "\n",
    "# Print the output\n",
    "print(\"Output:\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66efd122",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def get_variable_info(*args):\n",
    "    variable_info = {}\n",
    "    \n",
    "    def get_info(arg):\n",
    "        info = {}\n",
    "        if isinstance(arg, torch.Tensor):\n",
    "            info[\"type\"] = \"Tensor\"\n",
    "            info[\"shape\"] = tuple(arg.shape)\n",
    "        elif isinstance(arg, (list, tuple)):\n",
    "            info[\"length\"] = len(arg)\n",
    "            if arg:  # Check if the list or tuple is not empty\n",
    "                item_shapes = []\n",
    "                for item in arg:\n",
    "                    item_shapes.append(get_info(item))\n",
    "                info[\"item_shapes\"] = item_shapes\n",
    "        else:\n",
    "            info[\"type\"] = type(arg).__name__\n",
    "        return info\n",
    "    \n",
    "    for i, arg_value in enumerate(args):\n",
    "        arg_name = f\"arg_{i+1}\"  # Create a name for the argument\n",
    "        variable_info[arg_name] = get_info(arg_value)\n",
    "    \n",
    "    return variable_info\n",
    "\n",
    "# Example usage\n",
    "x = torch.randn(1, 3, 16, 256, 320)\n",
    "y = torch.randn(2, 3, 4)\n",
    "z = [torch.randn(2, 3), torch.randn(3, 4, 5)]\n",
    "\n",
    "info = get_variable_info(x, y, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd64af9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671fd858",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class Foo(torch.nn.Module):\n",
    "    def forward(self, tensor):\n",
    "        # It is data dependent\n",
    "        # Trace will only work with one path\n",
    "        if tensor.max() > 0.5:\n",
    "            return tensor ** 2\n",
    "        return tensor\n",
    "\n",
    "\n",
    "model = Foo()\n",
    "traced = torch.jit.script(model) # No warnings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6956d99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "traced_v2= torch.jit.trace(model, torch.randn(10)) # Warning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747bb327",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = torch.randn(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5448a9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = traced(test_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e02fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f593b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "traced_v2(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3a01f304",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = torch.nn.Linear(20, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1333a154",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 20])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m._parameters['weight'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5ee4b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
