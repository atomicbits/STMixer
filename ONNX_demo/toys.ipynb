{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3e0df04",
   "metadata": {},
   "source": [
    "## Goal \n",
    "Test different scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31325aff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f1538fb4c10>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import onnx\n",
    "import onnxruntime\n",
    "\n",
    "from typing import Any, List\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "torch.manual_seed(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891183e5",
   "metadata": {},
   "source": [
    "### Some thoughts on eager-mode and JIT\n",
    "\n",
    "Some links: \n",
    "1. https://backtrace.blog/2023/05/21/understanding-pytorch-eager-and-graph-mode/\n",
    "2. https://www.goldsborough.me/ml/ai/python/2018/02/04/20-17-20-a_promenade_of_pytorch/\n",
    "\n",
    "\n",
    "#### Torch ()\n",
    "   * **Eager Mode (Define-by-run)**:  the computational graph of a model is defined on the fly as the operations are run. Eager execution allows for dynamic control flow in the network, including loops, ifs, and other Python control structures. \n",
    "   * **Graph Mode (Define-and-run)**: Graph mode, on the other hand, builds the entire computation graph before running the computation. The graph-based execution in PyTorch is provided via **TorchScript**, which uses a Just-In-Time (JIT) compiler to convert PyTorch models to a graph representation. The major advantage of this mode is that it can perform various optimizations to speed up execution, and it allows models to be run in non-Python environments, which is crucial for deployment scenarios.\n",
    "\n",
    "#### Torch vs TensforFlow w.r.t control flow. \n",
    "In a static graph environment of TF, control flow must be represented as specialized nodes in the graph. For example, to enable branching, Tensorflow has a tf.cond() operation, which takes three subgraphs as input: a condition subgraph and two subgraphs for the if and else branches of the conditional. Similarly, loops must be represented in TensorFlow graphs as tf.while() operations, taking a condition and body subgraph as input. This operation done first to construct the graph before running it.\n",
    "In PyTorch, the computational graph is created dynamically at runtime, when you actually run data through the model. So when you execute output = model(input), the operations defined in your model’s forward() function are performed on the input data, and the computational graph is built as these operations are executed.\n",
    "\n",
    "#### Question: So, how dynamic control flow is handled in torch?\n",
    "When you call model(input) again, PyTorch will create a new computational graph. This is because, in PyTorch’s “define-by-run” paradigm, the computational graph is built dynamically each time you perform a forward pass with its new data. \n",
    "\n",
    "**Note that** , the graph can potentially be different from the one in the first forward pass, since the operations can be dynamic (e.g., they can depend on Python control flow like if statements or for loops).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d61d00",
   "metadata": {},
   "source": [
    "## Test Case 1: data dependent control flow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "435feaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Foo(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Foo, self).__init__()\n",
    "    def forward(self, x):\n",
    "        # It is data dependent and Trace will only work with one path\n",
    "        if x.min() > 0:\n",
    "            return torch.square(x) \n",
    "        else: \n",
    "            return -1 * torch.square(x)\n",
    "\n",
    "    \n",
    "#def FoF(x):\n",
    "#    return torch.sqrt(x) if x.max() > 0 else torch.square(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65c8ecd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = Foo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f6a814a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_tensor = torch.rand(1, 10)\n",
    "\n",
    "positive_tensor = sample_tensor.clone()\n",
    "negative_tensor = sample_tensor.clone()\n",
    "\n",
    "# Make all elements of the positive tensor positive\n",
    "positive_tensor[positive_tensor < 0] *= -1\n",
    "\n",
    "# Make all elements of the negative tensor negative\n",
    "negative_tensor[negative_tensor > 0] *= -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4652c623",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.4963, 0.7682, 0.0885, 0.1320, 0.3074, 0.6341, 0.4901, 0.8964, 0.4556,\n",
       "          0.6323]]),\n",
       " tensor([[-0.4963, -0.7682, -0.0885, -0.1320, -0.3074, -0.6341, -0.4901, -0.8964,\n",
       "          -0.4556, -0.6323]]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_tensor, negative_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a8e4e02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.2463, 0.5902, 0.0078, 0.0174, 0.0945, 0.4021, 0.2402, 0.8036, 0.2076,\n",
       "          0.3998]]),\n",
       " tensor([[-0.2463, -0.5902, -0.0078, -0.0174, -0.0945, -0.4021, -0.2402, -0.8036,\n",
       "          -0.2076, -0.3998]]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_output = foo(positive_tensor)\n",
    "negative_output = foo(negative_tensor)\n",
    "\n",
    "positive_output, negative_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df58226",
   "metadata": {},
   "source": [
    "### tracing\n",
    "Apparantly tracing does not work.\n",
    "\n",
    "In tracing, it run the code, record the operations that happen and construct a ScriptModule that does exactly that. Unfortunately, things like control flow are erased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "daa07a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2053/626634658.py:6: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if x.min() > 0:\n"
     ]
    }
   ],
   "source": [
    "traced_class = torch.jit.trace(Foo(), positive_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7db7ab4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.jit._trace.TopLevelTracedModule"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(traced_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36f706f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def forward(self,\\n    x: Tensor) -> Tensor:\\n  return torch.square(x)\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traced_class.code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b358afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph(%self : __torch__.Foo,\n",
      "      %x : Float(1, 10, strides=[10, 1], requires_grad=0, device=cpu)):\n",
      "  %7 : Float(1, 10, strides=[10, 1], requires_grad=0, device=cpu) = aten::square(%x) # /tmp/ipykernel_2053/626634658.py:7:0\n",
      "  return (%7)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(traced_class.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96a17bef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2463, 0.5902, 0.0078, 0.0174, 0.0945, 0.4021, 0.2402, 0.8036, 0.2076,\n",
       "         0.3998]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traced_class(negative_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eee6c32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scripted_class = torch.jit.script(foo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a44ae8d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def forward(self,\n",
      "    x: Tensor) -> Tensor:\n",
      "  if bool(torch.gt(torch.min(x), 0)):\n",
      "    _0 = torch.square(x)\n",
      "  else:\n",
      "    _0 = torch.mul(torch.square(x), -1)\n",
      "  return _0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(scripted_class.code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97a10552",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2463, -0.5902, -0.0078, -0.0174, -0.0945, -0.4021, -0.2402, -0.8036,\n",
       "         -0.2076, -0.3998]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scripted_class(negative_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e27ce7a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported graph: graph(%x.1 : Float(1, 10, strides=[10, 1], requires_grad=0, device=cpu)):\n",
      "  %/ReduceMin_output_0 : Float(device=cpu) = onnx::ReduceMin[keepdims=0, onnx_name=\"/ReduceMin\"](%x.1), scope: Foo:: # /tmp/ipykernel_2053/626634658.py:6:11\n",
      "  %/Constant_output_0 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={0}, onnx_name=\"/Constant\"](), scope: Foo:: # /tmp/ipykernel_2053/626634658.py:6:11\n",
      "  %/Greater_output_0 : Bool(device=cpu) = onnx::Greater[onnx_name=\"/Greater\"](%/ReduceMin_output_0, %/Constant_output_0), scope: Foo:: # /tmp/ipykernel_2053/626634658.py:6:11\n",
      "  %/Cast_output_0 : Bool(device=cpu) = onnx::Cast[to=9, onnx_name=\"/Cast\"](%/Greater_output_0), scope: Foo:: # /tmp/ipykernel_2053/626634658.py:6:8\n",
      "  %5 : Float(1, 10, strides=[10, 1], device=cpu) = onnx::If[onnx_name=\"/If\"](%/Cast_output_0), scope: Foo:: # /tmp/ipykernel_2053/626634658.py:6:8\n",
      "    block0():\n",
      "      %/Mul_output_0 : Float(1, 10, strides=[10, 1], device=cpu) = onnx::Mul[onnx_name=\"/Mul\"](%x.1, %x.1), scope: Foo:: # /tmp/ipykernel_2053/626634658.py:7:19\n",
      "      -> (%/Mul_output_0)\n",
      "    block1():\n",
      "      %/Mul_1_output_0 : Float(1, 10, strides=[10, 1], device=cpu) = onnx::Mul[onnx_name=\"/Mul_1\"](%x.1, %x.1), scope: Foo:: # /tmp/ipykernel_2053/626634658.py:9:24\n",
      "      %/Constant_1_output_0 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={-1}, onnx_name=\"/Constant_1\"](), scope: Foo:: # <string>:3:9\n",
      "      %/Mul_2_output_0 : Float(1, 10, strides=[10, 1], device=cpu) = onnx::Mul[onnx_name=\"/Mul_2\"](%/Mul_1_output_0, %/Constant_1_output_0), scope: Foo:: # <string>:3:9\n",
      "      -> (%/Mul_2_output_0)\n",
      "  return (%5)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.onnx.export(scripted_class, positive_tensor, \"./toy_models/foo_model_scripted.onnx\", verbose=True, opset_version=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5c520ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "ort_session = onnxruntime.InferenceSession(\"./toy_models/foo_model_scripted.onnx\")\n",
    "\n",
    "# Run inference with sample input\n",
    "ort_input_value = negative_tensor.detach().numpy()\n",
    "ort_inputs = {ort_session.get_inputs()[0].name: ort_input_value}\n",
    "ort_outputs = ort_session.run(None, ort_inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "85b64fc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.2462706 , -0.5901647 , -0.00782826, -0.01743205, -0.09450879,\n",
       "         -0.40205577, -0.24019155, -0.8036132 , -0.20759685, -0.39981124]],\n",
       "       dtype=float32)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ort_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da97729",
   "metadata": {},
   "source": [
    "**observation**: onnx on scripted class works but fails on traced class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "288ec8a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported graph: graph(%x : Float(1, 10, strides=[10, 1], requires_grad=0, device=cpu)):\n",
      "  %1 : Float(1, 10, strides=[10, 1], requires_grad=0, device=cpu) = onnx::Mul[onnx_name=\"/Mul\"](%x, %x), scope: Foo:: # /tmp/ipykernel_2053/626634658.py:7:0\n",
      "  return (%1)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torch/onnx/utils.py:823: UserWarning: no signature found for <torch.ScriptMethod object at 0x7f150eb16270>, skipping _decide_input_format\n",
      "  warnings.warn(f\"{e}, skipping _decide_input_format\")\n"
     ]
    }
   ],
   "source": [
    "torch.onnx.export(traced_class, positive_tensor, \"./toy_models/foo_model_traced.onnx\", verbose=True, opset_version=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "87bca749",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.2462706 , 0.5901647 , 0.00782826, 0.01743205, 0.09450879,\n",
       "         0.40205577, 0.24019155, 0.8036132 , 0.20759685, 0.39981124]],\n",
       "       dtype=float32)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ort_session = onnxruntime.InferenceSession(\"./toy_models/foo_model_traced.onnx\")\n",
    "\n",
    "# Run inference with sample input\n",
    "ort_input_value = negative_tensor.detach().numpy()\n",
    "ort_inputs = {ort_session.get_inputs()[0].name: ort_input_value}\n",
    "ort_outputs = ort_session.run(None, ort_inputs)\n",
    "ort_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4b2210",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ce5518",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c747a568",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66fe462",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb6ec39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121d7a05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631b74bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f7a9c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d8024b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModule(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModule, self).__init__()\n",
    "        # Define your layers here\n",
    "        self.linear = torch.nn.Linear(10, 5)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, x_tensor, y_integer):\n",
    "        # Your forward pass logic here\n",
    "        x = self.linear(x_tensor)\n",
    "        x = self.relu(x)\n",
    "        x = x * y_integer  # You can perform operations with the integer input\n",
    "        return x\n",
    "\n",
    "# Instantiate your model\n",
    "model = MyModule()\n",
    "\n",
    "# Example input\n",
    "input_tensor = torch.randn(1, 10)\n",
    "input_integer = 3\n",
    "\n",
    "# Trace the model\n",
    "traced_model = torch.jit.trace(model, (input_tensor, input_integer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9655f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adapter(inputs):\n",
    "    \"\"\"\n",
    "    Convert inputs to tensors if they are not already tensors.\n",
    "\n",
    "    Args:\n",
    "    inputs (list): List of inputs.\n",
    "\n",
    "    Returns:\n",
    "    list: List of tensors.\n",
    "    \"\"\"\n",
    "    tensor_inputs = []\n",
    "    for item in inputs:\n",
    "        if not isinstance(item, torch.Tensor):\n",
    "            item = torch.tensor(item)\n",
    "        tensor_inputs.append(item)\n",
    "    return tuple(tensor_inputs)\n",
    "\n",
    "# Example usage:\n",
    "input_list = [torch.randn(1, 10), 1]\n",
    "output_list = adapter(input_list)\n",
    "print(\"Output List of Tensors:\", output_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794f0771",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.jit.trace(model, adapter(input_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e977c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "model(*adapter(input_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1541d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x0, x1 = adapter(input_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1b448a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7051c064",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e32190",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48865c35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab46b24d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399e39e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "model = Foo()\n",
    "\n",
    "# Create sample input tensor\n",
    "small_tensor = torch.rand(1, 10) * 0.49\n",
    "print(\"small_tensor max is smaller than threshold: \", small_tensor.max() < 0.5)\n",
    "\n",
    "large_tensor = torch.rand(1, 10) + 0.5\n",
    "print(\"large_tensor max is smaller than threshold: \", large_tensor.max() < 0.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c31b5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "traced_model = torch.jit.trace(Foo(), small_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14888828",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(traced_model.code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08fc87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "scripted_model = torch.jit.script(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c5c3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scripted_model.code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332d7c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.onnx.export(model, small_tensor, \"foo_model.onnx\", verbose=True, opset_version=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec94a412",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.onnx.export(traced_model, small_tensor, \"foo_model_traced.onnx\", verbose=True, opset_version=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d73d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.onnx.export(scripted_model, small_tensor, \"foo_model_scripted.onnx\", verbose=True, opset_version=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b6185c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8207c9aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c6215a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01b74e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, List\n",
    "import torch\n",
    "\n",
    "class TestModule(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> Any:\n",
    "        ret: List[torch.Tensor] = []\n",
    "        if input.shape[0] == 1:\n",
    "            return input\n",
    "        else:\n",
    "            ret.append(input)\n",
    "            return ret\n",
    "\n",
    "m = TestModule()\n",
    "m_scripted = torch.jit.script(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f4b75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(m_scripted.code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05267843",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = torch.nn.ModuleList([\n",
    "            torch.nn.Linear(32, 32),\n",
    "            torch.nn.Linear(32, 32)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x, n_layers: int):\n",
    "        for i in range(n_layers):\n",
    "            x = self.layers[i](x)\n",
    "        return x\n",
    "\n",
    "torch.jit.script(Model())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fa7572",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import ModuleList\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MyEncoderLayer(torch.nn.Module):\n",
    "    def __init__(self, d=32):\n",
    "        super().__init__()\n",
    "        self.d = d\n",
    "        self.empty = False\n",
    "        self.layer = torch.nn.Linear(d, d)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        if torch.sum(x) > 0:\n",
    "            return self.layer(x)\n",
    "        return self.layer(2 * x)\n",
    "\n",
    "class MyModule(torch.nn.Module):\n",
    "    def __init__(self, d):\n",
    "        super().__init__()\n",
    "        self.d = d\n",
    "        self.empty = False\n",
    "        self.layers = torch.nn.ModuleList([MyEncoderLayer(32), MyEncoderLayer(32)])\n",
    "\n",
    "    def forward(self, x: torch.Tensor, c: int):\n",
    "        result = torch.rand(self.d)\n",
    "        for i, submodule in enumerate(self.layers):\n",
    "            if i < c and not submodule.empty:\n",
    "                result = submodule.forward(x)\n",
    "        return result \n",
    "\n",
    "d = 32\n",
    "script = torch.jit.script(MyModule(d))\n",
    "out = script.forward(torch.randn(d), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d54bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, x: torch.Tensor, c: int):\n",
    "        result = torch.rand(self.d)\n",
    "        for i, submodule in enumerate(self.layers):\n",
    "            if i < c and not submodule.empty:\n",
    "                result = submodule.forward(x)\n",
    "        return result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508fc169",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import onnx\n",
    "import onnxruntime\n",
    "\n",
    "\n",
    "@torch.jit.script\n",
    "def status(x):\n",
    "    if x.max() > 0.5:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "\n",
    "class Foo(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Foo, self).__init__()\n",
    "    def forward(self, tensor):\n",
    "        # It is data dependent and Trace will only work with one path\n",
    "        if status(tensor):\n",
    "            return tensor ** 2\n",
    "        return tensor\n",
    "\n",
    "# Instantiate the model\n",
    "model = Foo()\n",
    "\n",
    "# Create sample input tensor\n",
    "small_tensor = torch.rand(1, 10) * 0.5\n",
    "print(\"small_tensor max is smaller than threshold: \", small_tensor.max() < 0.5)\n",
    "\n",
    "large_tensor = torch.rand(1, 10) + 0.5\n",
    "print(\"large_tensor max is smaller than threshold: \", large_tensor.max() < 0.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496e5f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "model(small_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37416d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "scripted_Foo = torch.jit.script(Foo())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999f1c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scripted_Foo.code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e21684",
   "metadata": {},
   "outputs": [],
   "source": [
    "scripted_Foo(small_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e09a96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import onnx\n",
    "import onnxruntime\n",
    "\n",
    "@torch.jit.export\n",
    "def status(x):\n",
    "    if x.max() > 0.5:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "\n",
    "class Foo(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Foo, self).__init__()\n",
    "    def forward(self, tensor):\n",
    "        # It is data dependent and Trace will only work with one path\n",
    "        if status(tensor):\n",
    "            return tensor ** 2\n",
    "        return tensor\n",
    "\n",
    "# Instantiate the model\n",
    "model = Foo()\n",
    "\n",
    "# Create sample input tensor\n",
    "small_tensor = torch.rand(1, 10) * 0.5\n",
    "print(\"small_tensor max is smaller than threshold: \", small_tensor.max() < 0.5)\n",
    "\n",
    "large_tensor = torch.rand(1, 10) + 0.5\n",
    "print(\"large_tensor max is smaller than threshold: \", large_tensor.max() < 0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6a9b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "scripted_Foo = torch.jit.script(Foo())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb2baa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "scripted_Foo(small_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17dc3f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b93f584",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_tensor_output = model(small_tensor)\n",
    "small_tensor_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dec64c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "large_tensor_output = model(large_tensor)\n",
    "large_tensor_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf3c5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trace the model using example input\n",
    "#traced_model = torch.jit.trace(model, small_tensor)\n",
    "if False:\n",
    "    # Export the traced model to ONNX\n",
    "    torch.onnx.export(model, small_tensor, \"foo_model.onnx\", verbose=True, opset_version=17)\n",
    "\n",
    "    # Load the ONNX model using ONNX Runtime\n",
    "    ort_session = onnxruntime.InferenceSession(\"foo_model.onnx\")\n",
    "\n",
    "    # Run inference with sample input\n",
    "    ort_input_value = small_tensor.detach().numpy()\n",
    "    ort_inputs = {ort_session.get_inputs()[0].name: ort_input_value}\n",
    "    ort_outputs = ort_session.run(None, ort_inputs)\n",
    "\n",
    "    print(\"Output:\", ort_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48843ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    # Export the traced model to ONNX\n",
    "    torch.onnx.export(model, small_tensor, \"l_foo_model.onnx\", verbose=True, opset_version=17)\n",
    "\n",
    "    # Load the ONNX model using ONNX Runtime\n",
    "    ort_session = onnxruntime.InferenceSession(\"l_foo_model.onnx\")\n",
    "\n",
    "    # Run inference with sample input\n",
    "    ort_input_value = large_tensor.detach().numpy()\n",
    "    ort_inputs = {ort_session.get_inputs()[0].name: ort_input_value}\n",
    "    ort_outputs = ort_session.run(None, ort_inputs)\n",
    "\n",
    "    print(\"Output:\", ort_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa1e429",
   "metadata": {},
   "outputs": [],
   "source": [
    "large_tensor_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08912d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "large_tensor_output ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d110cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.jit.script(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e56291",
   "metadata": {},
   "outputs": [],
   "source": [
    "traced_cell = torch.jit.trace(model, small_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d948a286",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76de305f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b40718",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c5550c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f523327",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251d0d11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8d740d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb62fee5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d26773",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "import onnx.numpy_helper as np_helper\n",
    "import numpy as np\n",
    "\n",
    "from onnx import helper, shape_inference\n",
    "\n",
    "# Define the input tensor shape\n",
    "input_shape = (10,)\n",
    "\n",
    "# Create the input tensor and maximum value\n",
    "input_tensor = np.random.rand(*input_shape).astype(np.float32)\n",
    "max_value = np.max(input_tensor)\n",
    "\n",
    "# Check if the maximum value is greater than 0.5\n",
    "output_value = max_value if max_value > 0.5 else None\n",
    "\n",
    "# Create the input and output names\n",
    "input_name = \"input\"\n",
    "output_name = \"output\"\n",
    "\n",
    "# Create the input tensor value info\n",
    "input_tensor_value_info = helper.make_tensor_value_info(input_name, onnx.TensorProto.FLOAT, input_shape)\n",
    "\n",
    "# Create the output tensor value info\n",
    "output_tensor_value_info = helper.make_empty_tensor_value_info(output_name)\n",
    "\n",
    "# Create the conditional node\n",
    "if output_value is not None:\n",
    "    constant_node = helper.make_node(\n",
    "        \"Constant\",\n",
    "        inputs=[],\n",
    "        outputs=[\"condition\"],\n",
    "        value=np_helper.from_array(np.array([output_value], dtype=np.float32)),\n",
    "    )\n",
    "\n",
    "    greater_node = helper.make_node(\n",
    "        \"Greater\",\n",
    "        inputs=[input_name, \"condition\"],\n",
    "        outputs=[output_name],\n",
    "    )\n",
    "else:\n",
    "    # If maximum value is not greater than 0.5, output should be omitted\n",
    "    output_tensor_value_info = None\n",
    "\n",
    "# Create the graph\n",
    "if output_tensor_value_info is not None:\n",
    "    graph = helper.make_graph(\n",
    "        [constant_node, greater_node],\n",
    "        \"max_value_graph\",\n",
    "        [input_tensor_value_info],\n",
    "        [output_tensor_value_info],\n",
    "    )\n",
    "else:\n",
    "    graph = helper.make_graph(\n",
    "        [],\n",
    "        \"max_value_graph\",\n",
    "        [input_tensor_value_info],\n",
    "        [],\n",
    "    )\n",
    "\n",
    "# Perform shape inference\n",
    "inferred_graph = shape_inference.infer_shapes(graph)\n",
    "\n",
    "# Create the model\n",
    "onnx_model = helper.make_model(inferred_graph)\n",
    "\n",
    "# Save the ONNX model to a file\n",
    "onnx.save(onnx_model, \"tpy_model.onnx\")\n",
    "\n",
    "print(\"ONNX model saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af2ac07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime\n",
    "\n",
    "ort_session = onnxruntime.InferenceSession(onnx_model_path)\n",
    "\n",
    "# Generate input tensor\n",
    "input_tensor = np.random.rand(10).astype(np.float32)\n",
    "\n",
    "# Run inference\n",
    "input_name = ort_session.get_inputs()[0].name\n",
    "output_name = ort_session.get_outputs()[0].name\n",
    "output = ort_session.run([output_name], {input_name: input_tensor.reshape(1, -1)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f064fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "import onnxruntime\n",
    "import numpy as np\n",
    "\n",
    "# Define the input shape\n",
    "input_shape = (1, 10)\n",
    "\n",
    "# Create the input tensor with random values\n",
    "input_data = np.random.rand(*input_shape).astype(np.float32)\n",
    "\n",
    "# Create the maximum value threshold\n",
    "threshold = np.array([0.5], dtype=np.float32)\n",
    "\n",
    "# Create the ONNX model\n",
    "node_input = onnx.helper.make_tensor_value_info('input', onnx.TensorProto.FLOAT, input_shape)\n",
    "node_threshold = onnx.helper.make_tensor_value_info('threshold', onnx.TensorProto.FLOAT, [1])\n",
    "node_output = onnx.helper.make_tensor_value_info('output', onnx.TensorProto.FLOAT, [1])\n",
    "\n",
    "nodes = [\n",
    "    onnx.helper.make_node('Max', inputs=['input'], outputs=['max_value']),\n",
    "    onnx.helper.make_node('Greater', inputs=['max_value', 'threshold'], outputs=['greater_than_threshold']),\n",
    "    onnx.helper.make_node('Cast', inputs=['greater_than_threshold'], outputs=['cast_output'], to=9),  # Cast to boolean (9)\n",
    "    onnx.helper.make_node('Where', inputs=['cast_output', 'max_value', 'threshold'], outputs=['output'])\n",
    "]\n",
    "\n",
    "graph_def = onnx.helper.make_graph(nodes, 'max_value_graph', [node_input, node_threshold], [node_output])\n",
    "\n",
    "model_def = onnx.helper.make_model(graph_def, producer_name='max_value_model')\n",
    "\n",
    "# Save the ONNX model to a file\n",
    "onnx.save(model_def, 'max_value_model.onnx')\n",
    "\n",
    "# Run inference using ONNX Runtime\n",
    "ort_session = onnxruntime.InferenceSession('max_value_model.onnx')\n",
    "outputs = ort_session.run(None, {'input': input_data, 'threshold': threshold})\n",
    "\n",
    "print(\"Output:\", outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90514f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "test_input = torch.randn(10)\n",
    "\n",
    "class Foo(torch.nn.Module):\n",
    "    def forward(self, tensor):\n",
    "        # It is data dependent\n",
    "        # Trace will only work with one path\n",
    "        if tensor.max() > 0.5:\n",
    "            return tensor ** 2\n",
    "        return tensor\n",
    "\n",
    "\n",
    "model = Foo()\n",
    "model_scripted_v1 = torch.jit.script(model) # No warnings\n",
    "model_scripted_v2 = torch.jit.trace(model, test_input) # Warning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667024be",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5caaeac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = np.random.rand(*input_shape).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8f81dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ort_session = onnxruntime.InferenceSession('max_value_model.onnx')\n",
    "outputs = ort_session.run(None, {'input': input_data, 'threshold': threshold})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25704f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b62a99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_0 = np.array([3, 2, 1]).astype(np.float32)\n",
    "data_1 = np.array([1, 4, 4]).astype(np.float32)\n",
    "data_2 = np.array([2, 5, 3]).astype(np.float32)\n",
    "result = np.array([3, 5, 4]).astype(np.float32)\n",
    "node = onnx.helper.make_node(\n",
    "    \"Max\",\n",
    "    inputs=[\"data_0\", \"data_1\", \"data_2\"],\n",
    "    outputs=[\"result\"],\n",
    ")\n",
    "expect(\n",
    "    node,\n",
    "    inputs=[data_0, data_1, data_2],\n",
    "    outputs=[result],\n",
    "    name=\"test_max_example\",\n",
    ")\n",
    "\n",
    "node = onnx.helper.make_node(\n",
    "    \"Max\",\n",
    "    inputs=[\"data_0\"],\n",
    "    outputs=[\"result\"],\n",
    ")\n",
    "expect(node, inputs=[data_0], outputs=[data_0], name=\"test_max_one_input\")\n",
    "\n",
    "result = np.maximum(data_0, data_1)\n",
    "node = onnx.helper.make_node(\n",
    "    \"Max\",\n",
    "    inputs=[\"data_0\", \"data_1\"],\n",
    "    outputs=[\"result\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf76fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Sequence\n",
    "import numpy as np\n",
    "import onnx\n",
    "import onnxruntime\n",
    "import subprocess\n",
    "import sys\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "from typing import Any, Callable, Dict, List, Optional, Sequence, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import onnx\n",
    "from onnx.backend.test.case.test_case import TestCase\n",
    "from onnx.backend.test.case.utils import import_recursive\n",
    "from onnx.onnx_pb import (\n",
    "    AttributeProto,\n",
    "    FunctionProto,\n",
    "    GraphProto,\n",
    "    ModelProto,\n",
    "    NodeProto,\n",
    "    TensorProto,\n",
    "    TypeProto,\n",
    ")\n",
    "\n",
    "_NodeTestCases = []\n",
    "_TargetOpType = None\n",
    "_DiffOpTypes = None\n",
    "\n",
    "\n",
    "def _extract_value_info(\n",
    "    input: Union[List[Any], np.ndarray, None],\n",
    "    name: str,\n",
    "    type_proto: Optional[TypeProto] = None,\n",
    ") -> onnx.ValueInfoProto:\n",
    "    if type_proto is None:\n",
    "        if input is None:\n",
    "            raise NotImplementedError(\n",
    "                \"_extract_value_info: both input and type_proto arguments cannot be None.\"\n",
    "            )\n",
    "        elif isinstance(input, list):\n",
    "            elem_type = onnx.helper.np_dtype_to_tensor_dtype(input[0].dtype)\n",
    "            shape = None\n",
    "            tensor_type_proto = onnx.helper.make_tensor_type_proto(elem_type, shape)\n",
    "            type_proto = onnx.helper.make_sequence_type_proto(tensor_type_proto)\n",
    "        elif isinstance(input, TensorProto):\n",
    "            elem_type = input.data_type\n",
    "            shape = tuple(input.dims)\n",
    "            type_proto = onnx.helper.make_tensor_type_proto(elem_type, shape)\n",
    "        else:\n",
    "            elem_type = onnx.helper.np_dtype_to_tensor_dtype(input.dtype)\n",
    "            shape = input.shape\n",
    "            type_proto = onnx.helper.make_tensor_type_proto(elem_type, shape)\n",
    "\n",
    "    return onnx.helper.make_value_info(name, type_proto)\n",
    "\n",
    "def expect(\n",
    "    node: onnx.NodeProto,\n",
    "    inputs: Sequence[np.ndarray],\n",
    "    outputs: Sequence[np.ndarray],\n",
    "    name: str,\n",
    "    **kwargs: Any,\n",
    ") -> None:\n",
    "    # Builds the model\n",
    "    present_inputs = [x for x in node.input if (x != \"\")]\n",
    "    present_outputs = [x for x in node.output if (x != \"\")]\n",
    "    input_type_protos = [None] * len(inputs)\n",
    "    if \"input_type_protos\" in kwargs:\n",
    "        input_type_protos = kwargs[\"input_type_protos\"]\n",
    "        del kwargs[\"input_type_protos\"]\n",
    "    output_type_protos = [None] * len(outputs)\n",
    "    if \"output_type_protos\" in kwargs:\n",
    "        output_type_protos = kwargs[\"output_type_protos\"]\n",
    "        del kwargs[\"output_type_protos\"]\n",
    "    inputs_vi = [\n",
    "        _extract_value_info(arr, arr_name, input_type)\n",
    "        for arr, arr_name, input_type in zip(inputs, present_inputs, input_type_protos)\n",
    "    ]\n",
    "    outputs_vi = [\n",
    "        _extract_value_info(arr, arr_name, output_type)\n",
    "        for arr, arr_name, output_type in zip(\n",
    "            outputs, present_outputs, output_type_protos\n",
    "        )\n",
    "    ]\n",
    "    graph = onnx.helper.make_graph(\n",
    "        nodes=[node], name=name, inputs=inputs_vi, outputs=outputs_vi\n",
    "    )\n",
    "    kwargs[\"producer_name\"] = \"backend-test\"\n",
    "\n",
    "    if \"opset_imports\" not in kwargs:\n",
    "        # To make sure the model will be produced with the same opset_version after opset changes\n",
    "        # By default, it uses since_version as opset_version for produced models\n",
    "        produce_opset_version = onnx.defs.get_schema(\n",
    "            node.op_type, domain=node.domain\n",
    "        ).since_version\n",
    "        kwargs[\"opset_imports\"] = [\n",
    "            onnx.helper.make_operatorsetid(node.domain, produce_opset_version)\n",
    "        ]\n",
    "\n",
    "    model = onnx.helper.make_model_gen_version(graph, **kwargs)\n",
    "\n",
    "    # Checking the produces are the expected ones.\n",
    "    sess = onnxruntime.InferenceSession(model.SerializeToString(),\n",
    "                                        providers=[\"CPUExecutionProvider\"])\n",
    "    feeds = {name: value for name, value in zip(node.input, inputs)}\n",
    "    results = sess.run(None, feeds)\n",
    "    for expected, output in zip(outputs, results):\n",
    "        return (results, outputs)\n",
    "        return np.testing.assert_allclose(expected, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7068715a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_0 = np.array([3, 2, 1]).astype(np.float32)\n",
    "data_1 = np.array([1, 4, 4]).astype(np.float32)\n",
    "data_2 = np.array([2, 5, 3]).astype(np.float32)\n",
    "result = np.array([3, 5, 4]).astype(np.float32)\n",
    "node = onnx.helper.make_node(\n",
    "    \"Max\",\n",
    "    inputs=[\"data_0\", \"data_1\", \"data_2\"],\n",
    "    outputs=[\"result\"],\n",
    ")\n",
    "expect(\n",
    "    node,\n",
    "    inputs=[data_0, data_1, data_2],\n",
    "    outputs=[result],\n",
    "    name=\"test_max_example\",\n",
    ")\n",
    "\n",
    "node = onnx.helper.make_node(\n",
    "    \"Max\",\n",
    "    inputs=[\"data_0\"],\n",
    "    outputs=[\"result\"],\n",
    ")\n",
    "expect(node, inputs=[data_0], outputs=[data_0], name=\"test_max_one_input\")\n",
    "\n",
    "result = np.maximum(data_0, data_1)\n",
    "node = onnx.helper.make_node(\n",
    "    \"Max\",\n",
    "    inputs=[\"data_0\", \"data_1\"],\n",
    "    outputs=[\"result\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de45bb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "expect(\n",
    "    node, inputs=[data_0, data_1], outputs=[result], name=\"test_max_two_inputs\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be69a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import onnx\n",
    "import onnxruntime\n",
    "\n",
    "class Foo(torch.nn.Module):\n",
    "    def forward(self, tensor):\n",
    "        # It is data dependent\n",
    "        # Trace will only work with one path\n",
    "        if tensor.max() < 0.5:\n",
    "            return tensor ** 2\n",
    "        return tensor\n",
    "\n",
    "# Instantiate the model\n",
    "model = Foo()\n",
    "\n",
    "# Create sample input tensor\n",
    "small_tensor = torch.rand(1, 10) * 0.5\n",
    "\n",
    "large_tensor = torch.rand(1, 10) + 0.5\n",
    "print(large_tensor)\n",
    "\n",
    "# Trace the model using example input\n",
    "#traced_model = torch.jit.trace(model, small_tensor)\n",
    "\n",
    "# Export the traced model to ONNX\n",
    "torch.onnx.export(model, small_tensor, \"foo_model.onnx\", verbose=True, opset_version=11)\n",
    "\n",
    "# Load the ONNX model using ONNX Runtime\n",
    "ort_session = onnxruntime.InferenceSession(\"foo_model.onnx\")\n",
    "\n",
    "# Run inference with sample input\n",
    "ort_inputs = {ort_session.get_inputs()[0].name: large_tensor.detach().numpy()}\n",
    "ort_outputs = ort_session.run(None, ort_inputs)\n",
    "\n",
    "print(\"Output:\", ort_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f03ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "large_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffa8f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32983444",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283af64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc71058f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tensor of shape (1, 10) with random values between 0 and 1\n",
    "tensor = torch.rand(1, 10)\n",
    "\n",
    "# Multiply the tensor by 0.5 to ensure all elements are smaller than 0.5\n",
    "tensor *= 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94543e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference with sample input\n",
    "ort_inputs = {ort_session.get_inputs()[0].name: tensor.detach().numpy()}\n",
    "ort_outputs = ort_session.run(None, ort_inputs)\n",
    "\n",
    "print(\"Output:\", ort_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee7c91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "node = onnx.helper.make_node(\n",
    "    \"Erf\",\n",
    "    inputs=[\"x\"],\n",
    "    outputs=[\"y\"],\n",
    ")\n",
    "\n",
    "x = np.random.randn(1, 3, 32, 32).astype(np.float32)\n",
    "y = np.vectorize(math.erf)(x).astype(np.float32)\n",
    "expect(node, inputs=[x], outputs=[y], name=\"test_erf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e55fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyRNNLoop(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyRNNLoop, self).__init__()\n",
    "        self.cell = torch.jit.trace(MyCell(scripted_gate), (x, h))\n",
    "\n",
    "    def forward(self, xs):\n",
    "        h, y = torch.zeros(3, 4), torch.zeros(3, 4)\n",
    "        for i in range(xs.size(0)):\n",
    "            y, h = self.cell(xs[i], h)\n",
    "        return y, h\n",
    "\n",
    "rnn_loop = torch.jit.script(MyRNNLoop())\n",
    "print(rnn_loop.code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f118832",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.jit.script(Foo()).code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cdd7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, h = torch.rand(3, 4), torch.rand(3, 4)\n",
    "print(scripted_cell(x, h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94266108",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
