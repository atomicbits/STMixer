{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41b98a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import cv2\n",
    "import math\n",
    "import numpy as np\n",
    "import moviepy \n",
    "import torch\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "from my_alphaction.config import cfg\n",
    "from my_alphaction.modeling.detector import build_detection_model\n",
    "from my_alphaction.utils.checkpoint import ActionCheckpointer\n",
    "from my_alphaction.utils.comm import get_world_size\n",
    "\n",
    "\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0dce733",
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_alphaction.modeling.stm_decoder.stm_decoder import STMDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8949f30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'VMAEv2'\n",
    "\n",
    "\n",
    "person_threshold = 0.6 # confidence threshold on actor. 0.6 is the defualt\n",
    "sampling_rate = 3 # sampling rate: 4 is the defualt\n",
    "top_k = 5 # number of actions per person\n",
    "video_path = '../input_dir/markt2_fight.mp4'\n",
    "\n",
    "slice_height = 800\n",
    "slice_width = 1000\n",
    "overlap_ratio = 0.1\n",
    "\n",
    "starting_frame_index = 100\n",
    "length_input = 200\n",
    "\n",
    "exp_dict = {'model_name': model_name,\n",
    "            'model_params': {'person_threshold': person_threshold, \n",
    "                             'sampling_rate': sampling_rate},\n",
    "            'orig_post_processing':{'top_k': top_k},\n",
    "            'aggregation': {'method': {}, \n",
    "                            'params': {}},\n",
    "            'video_path': video_path,\n",
    "            'slicing_params': {'slice_height': slice_height, \n",
    "                               'slice_width': slice_width, \n",
    "                               'overlap_ratio':overlap_ratio},\n",
    "            'video_params': {'st_frame_index': starting_frame_index, \n",
    "                             'length_input':length_input\n",
    "                             }\n",
    "           }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b2af1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_name == 'VMAEv2':\n",
    "    config_file = '../config_files/VMAEv2-ViTB-16x4.yaml'\n",
    "if model_name == 'VMAE':\n",
    "    config_file = '../config_files/VMAE-ViTB-16x4.yaml'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03f859eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.merge_from_file(config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5404bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change model weight path\n",
    "if model_name == 'VMAEv2':\n",
    "    cfg.merge_from_list([\"MODEL.WEIGHT\", \"../checkpoints/VMAEv2_ViTB_16x4.pth\"])\n",
    "if model_name == 'VMAE':\n",
    "    cfg.merge_from_list([\"MODEL.WEIGHT\", \"../checkpoints/VMAE_ViTB_16x4.pth\"])\n",
    "\n",
    "# change output dir\n",
    "cfg.merge_from_list([\"OUTPUT_DIR\", \"../output_dir/\"])\n",
    "\n",
    "# change person threshold\n",
    "cfg.merge_from_list([\"MODEL.STM.PERSON_THRESHOLD\", person_threshold])\n",
    "\n",
    "# change sampling rate\n",
    "cfg.merge_from_list([\"DATA.SAMPLING_RATE\", sampling_rate])\n",
    "\n",
    "# change path for data_dir\n",
    "cfg.merge_from_list([\"DATA.PATH_TO_DATA_DIR\", \"/work/ava\"])\n",
    "\n",
    "# folder name of annotations\n",
    "cfg.merge_from_list([\"AVA.ANNOTATION_DIR\", \"annotations/\"])\n",
    "\n",
    "# file name of  frame_lists\n",
    "cfg.merge_from_list([\"AVA.TRAIN_LISTS\", ['sample.csv']])\n",
    "cfg.merge_from_list([\"AVA.TEST_LISTS\", ['sample.csv']])\n",
    "\n",
    "# file name of predicted_bboxes\n",
    "cfg.merge_from_list([\"AVA.TRAIN_GT_BOX_LISTS\", ['ava_sample_predicted_boxes.csv']])\n",
    "cfg.merge_from_list([\"AVA.TEST_GT_BOX_LISTS\", ['ava_sample_predicted_boxes.csv']])\n",
    "\n",
    "# file name of exlusions\n",
    "cfg.merge_from_list([\"AVA.EXCLUSION_FILE\", 'ava_sample_train_excluded_timestamps_v2.2.csv'])\n",
    "\n",
    "# number of batches in test scenario\n",
    "cfg.merge_from_list([\"TEST.VIDEOS_PER_BATCH\", 1])\n",
    "\n",
    "# number of workers\n",
    "cfg.merge_from_list([\"DATALOADER.NUM_WORKERS\", 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c361820",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.ViT.USE_CHECKPOINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "edb318ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.merge_from_list([\"ViT.USE_CHECKPOINT\", False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1921c641",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.ViT.USE_CHECKPOINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0fa2125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cfg.DATALOADER.SIZE_DIVISIBILITY:  32\n",
      "cfg.DATA.SAMPLING_RATE:  3\n",
      "cfg.DATA.NUM_FRAMES:  16\n",
      "self_seq_len:  48\n",
      "cfg.MODEL.STM.ACTION_CLASSES:  80\n",
      "Augmentation params:  [0.45, 0.45, 0.45] [0.225, 0.225, 0.225] False\n",
      "scale and flip params [256] 1333 False\n"
     ]
    }
   ],
   "source": [
    "debug = True\n",
    "if debug:\n",
    "    # The shape of model input should be divisible into this. Otherwise, padding 0 to left and bottum. \n",
    "    print(\"cfg.DATALOADER.SIZE_DIVISIBILITY: \", cfg.DATALOADER.SIZE_DIVISIBILITY)\n",
    "    \n",
    "    # Sampling rate in constructing the clips.\n",
    "    self_sample_rate =  cfg.DATA.SAMPLING_RATE\n",
    "    print(\"cfg.DATA.SAMPLING_RATE: \", cfg.DATA.SAMPLING_RATE)\n",
    "    \n",
    "    # Length of clip\n",
    "    self_video_length = cfg.DATA.NUM_FRAMES\n",
    "    print(\"cfg.DATA.NUM_FRAMES: \", cfg.DATA.NUM_FRAMES)\n",
    "    \n",
    "    # Length of sequence frames from which a clip is constructed.\n",
    "    self_seq_len = self_video_length * self_sample_rate\n",
    "    print(\"self_seq_len: \", self_seq_len)\n",
    "    \n",
    "    self_num_classes = cfg.MODEL.STM.ACTION_CLASSES\n",
    "    print(\"cfg.MODEL.STM.ACTION_CLASSES: \", self_num_classes)\n",
    "    \n",
    "    # Augmentation params.\n",
    "    self_data_mean = cfg.DATA.MEAN\n",
    "    self_data_std = cfg.DATA.STD\n",
    "    self_use_bgr = cfg.AVA.BGR\n",
    "    print(\"Augmentation params: \", self_data_mean, self_data_std, self_use_bgr)\n",
    "    \n",
    "    self_jitter_min_scale = cfg.DATA.TEST_MIN_SCALES\n",
    "    self_jitter_max_scale = cfg.DATA.TEST_MAX_SCALE\n",
    "    self_test_force_flip = cfg.AVA.TEST_FORCE_FLIP\n",
    "\n",
    "    print(\"scale and flip params\", self_jitter_min_scale, self_jitter_max_scale, self_test_force_flip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3beeaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "stm_decoder = STMDecoder(cfg\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e278383d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def create_random_variables(variable_info):\n",
    "    variables = {}\n",
    "    \n",
    "    def create_tensor(shape):\n",
    "        return torch.randn(*shape)\n",
    "    \n",
    "    for arg_name, info in variable_info.items():\n",
    "        if \"length\" in info:\n",
    "            # If the variable is a list\n",
    "            item_shapes = []\n",
    "            for item_info in info[\"item_shapes\"]:\n",
    "                item_shapes.append(create_tensor(item_info[\"shape\"]))\n",
    "            variables[arg_name] = item_shapes\n",
    "        elif \"shape\" in info:\n",
    "            # If the variable is a tensor\n",
    "            variables[arg_name] = create_tensor(info[\"shape\"])\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid variable info for {arg_name}\")\n",
    "    \n",
    "    return variables\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7216c670",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_variable_info = {'features': {'length': 4, 'item_shapes': [{'type': 'Tensor', 'shape': (1, 256, 8, 64, 80)}, {'type': 'Tensor', 'shape': (1, 256, 8, 32, 40)}, {'type': 'Tensor', 'shape': (1, 256, 8, 16, 20)}, {'type': 'Tensor', 'shape': (1, 256, 8, 8, 10)}]}, \n",
    "                 'proposal_boxes': {'type': 'Tensor', 'shape': (1, 100, 4)}, \n",
    "                 'spatial_queries': {'type': 'Tensor', 'shape': (1, 100, 256)}, \n",
    "                 'temporal_queries': {'type': 'Tensor', 'shape': (1, 100, 256)}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "324fc411",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_random_variables = create_random_variables(input_variable_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d40bb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = input_random_variables['features']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7b1810f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = stm_decoder(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "49996b36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 100, 2])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dda4d6b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/my_alphaction/modeling/stm_decoder/stm_decoder.py:381: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  whwh = torch.tensor(values, device='cpu')\n",
      "/work/my_alphaction/modeling/stm_decoder/stm_decoder.py:347: TracerWarning: Using len to get tensor shape might cause the trace to be incorrect. Recommended usage would be tensor.shape[0]. Passing a tensor of different shape might lead to errors or silently give incorrect results.\n",
      "  batch_size = len(whwh)\n",
      "/work/my_alphaction/modeling/stm_decoder/util/box_ops.py:206: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert (bboxes1.size(-1) == 4 or bboxes1.size(0) == 0)\n",
      "/work/my_alphaction/modeling/stm_decoder/util/box_ops.py:207: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert (bboxes2.size(-1) == 4 or bboxes2.size(0) == 0)\n",
      "/work/my_alphaction/modeling/stm_decoder/util/box_ops.py:211: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert bboxes1.shape[:-2] == bboxes2.shape[:-2]\n",
      "/work/my_alphaction/modeling/stm_decoder/util/box_ops.py:219: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if rows * cols == 0:\n",
      "/work/my_alphaction/modeling/stm_decoder/util/head_utils.py:45: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert token_xyzr.size(-1) == 4\n",
      "/work/my_alphaction/modeling/stm_decoder/util/msaq.py:100: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  mapping_size = value.new_tensor([value.size(4), value.size(3)]).view(1, 1, 1, 1, -1) * stride\n",
      "/work/my_alphaction/modeling/stm_decoder/util/adaptive_mixing_operator.py:58: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert g == G\n",
      "/usr/local/lib/python3.8/dist-packages/torch/onnx/_internal/jit_utils.py:258: UserWarning: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/passes/onnx/shape_type_inference.cpp:1891.)\n",
      "  _C._jit_pass_onnx_node_shape_type_inference(node, params_dict, opset_version)\n",
      "/usr/local/lib/python3.8/dist-packages/torch/onnx/utils.py:687: UserWarning: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/passes/onnx/shape_type_inference.cpp:1891.)\n",
      "  _C._jit_pass_onnx_graph_shape_type_inference(\n",
      "/usr/local/lib/python3.8/dist-packages/torch/onnx/utils.py:1178: UserWarning: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/passes/onnx/shape_type_inference.cpp:1891.)\n",
      "  _C._jit_pass_onnx_graph_shape_type_inference(\n"
     ]
    }
   ],
   "source": [
    "example_inputs = {\n",
    "    'features': [torch.randn(1, 256, 8, 64, 80),\n",
    "                 torch.randn(1, 256, 8, 32, 40),\n",
    "                 torch.randn(1, 256, 8, 16, 20),\n",
    "                 torch.randn(1, 256, 8, 8, 10)]\n",
    "}\n",
    "\n",
    "# Export the model to ONNX format\n",
    "torch.onnx.export(stm_decoder,\n",
    "                  features,\n",
    "                  'stm_decoder.onnx',\n",
    "                  input_names=['features'],\n",
    "                  output_names=['objectness_score'],\n",
    "                  \n",
    "                 opset_version=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc5af4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
